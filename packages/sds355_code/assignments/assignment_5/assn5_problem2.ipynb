{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5:  Topic Models\n",
    "\n",
    "> net id: sa857\n",
    "\n",
    "Due: Tuesday, November 5.\n",
    "\n",
    "This assignment has three problems. The first is about Bayesian inference. The second two are about topic models. You will first work with abstracts of scientific articles. These abstracts are obtained from [arXiv.org](http://arxiv.org), an open access repository for e-prints of articles in scientific fields maintained by Cornell University. You will then work with a collection of movie plots. \n",
    "\n",
    "*For your convenience, we have separated the problems into three notebooks: assn5_problem1.ipynb, assn5_problem2.ipynb, and assn5_problem3.ipynb. Submit your solutions in these three notebooks, printing out each as a separate pdf.*\n",
    "\n",
    "We provide significant \"starter code\" as discussed in lecture. We then ask you  build topic models using the Python library gensim, and do some analysis over the topics obtained.\n",
    "\n",
    "We ask that you please at least start the assignment right away. If you have any difficulties running gensim we would like to know!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Topic models on arXiv abstracts\n",
    "\n",
    "* The first part of this problem is mainly starter code, showing how to clean up the data. You only need to write some simple functions to remove specific types of words.\n",
    "\n",
    "* The starter code we provide uses \"list comprehensions\". Please see the code from lecture if you'd like to see the loop-based versions of this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.root.level = logging.CRITICAL \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# direct plots to appear within the cell, and set their style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstracts are in a JSON format in `abstracts.json`. Here is an excerpt from the data we'll be using.\n",
    "\n",
    "```\n",
    "{\n",
    "   \"1407.0001\": \"The topic of finding effective strategy to halt virus in complex network is of current interest. We propose an immunization strategy for seasonal epidemics that occur periodically. Based on the local information of the infection status from the previous epidemic season, the selection of vaccinated nodes is optimized gradually. The evolution of vaccinated nodes during iterations demonstrates that the immunization tends to locate in both global hubs and local hubs. We analyze the epidemic prevalence by a heterogeneous mean-field method and present numerical simulations of our model. This immunization performs superiorly to some other previously known strategies. Our work points out a new direction in immunization of seasonal epidemics.\", \n",
    "```\n",
    "\n",
    "We will read this using the `json` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('abstracts.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON is in a key-value format, where the key is a unique arXiv identifier, and the value is the abstract. We are only concerned with the abstracts, so we will extract them into an array.\n",
    "\n",
    "*In the next cell and subsequent cells, you will see code commented out. The commented out code has the same functionality as the code prior. The only difference between them is that the commented out ones are written in for loops, while the ones not commented are written using list comprehensions. If you study them, you should be able to understand why they are equivalent!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 50000\n",
      "\n",
      "\\noindent A statistical inconsistency of a zero-inflated Binomial likelihood model for count data is identified. This issue occurs when the response, $y$, is both zero and n-inflated, and results in statistically inconsistent and erroneous parameter inferences being drawn from the data. The zero-modified Binomial likelihood is amended to address this issue of \\textit{n-inflation}, resulting in a fully symmetric Binomial likelihood model for both zero and n-inflated counts. We present a simple regression example from the ecological literature which details the practical application of the new likelihood model.\n"
     ]
    }
   ],
   "source": [
    "sample = 30\n",
    "abstracts = [item[1] for item in data.items()]\n",
    "print(\"Number of documents: %d\\n\" % len(abstracts))\n",
    "print(abstracts[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This abstract is from article [arXiv:1407.0064v1](https://arxiv.org/abs/1407.0064v1) on the arXiv. From the sample document, we see that the abstracts can have various marked up text such as `\\noindent`, `$y$`, or `\\textit{n-inflation}`. This is LaTeX, a text markup system that allows for more convenient scientific technical writing. For example, the text `\\sum_{i=1}^ni = \\frac{n(n+1)}{2}` is processed to\n",
    "\n",
    "$$\\sum_{i=1}^ni = \\frac{n(n+1)}{2}$$\n",
    "\n",
    "LaTeX is widely used in academia across fields like statistics, computer science, linguistics and political science.\n",
    "\n",
    "For our purposes, we have to remove such markup as we only want to process natural text. We can once again use regular expressions to remove the markup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A statistical inconsistency of a zero inflated Binomial likelihood model for count data is identified This issue occurs when the response  is both zero and n inflated and results in statistically inconsistent and erroneous parameter inferences being drawn from the data The zero modified Binomial likelihood is amended to address this issue of n inflation resulting in a fully symmetric Binomial likelihood model for both zero and n inflated counts We present a simple regression example from the ecological literature which details the practical application of the new likelihood model\n"
     ]
    }
   ],
   "source": [
    "# remove $...$ or $$...$$ math markup\n",
    "abstracts = [re.sub(r'\\$\\$.*?\\$\\$', r'', abstract) for abstract in abstracts]\n",
    "abstracts = [re.sub(r'\\$.*?\\$', r'', abstract) for abstract in abstracts]\n",
    "\n",
    "# remove \\emph{...}, \\textit{...} type markup and replace with ...\n",
    "abstracts = [re.sub(r'\\\\\\S*?\\{(.*?)\\}', r'\\1', abstract) for abstract in abstracts]\n",
    "\n",
    "# remove \\noindent type markup\n",
    "abstracts = [re.sub(r'\\\\(\\S*)', r'', abstract) for abstract in abstracts]\n",
    "\n",
    "# remove {{...}}, {...}, and (...) markup, and replace with ...\n",
    "abstracts = [re.sub(r'\\{\\{(.*?)\\}\\}', r'\\1', abstract) for abstract in abstracts]\n",
    "abstracts = [re.sub(r'\\{(.*?)\\}', r'\\1', abstract) for abstract in abstracts]\n",
    "abstracts = [re.sub(r'\\((.*?)\\)', r'\\1', abstract) for abstract in abstracts]\n",
    "\n",
    "# replace '-' with ' ', then remove punctuation\n",
    "abstracts = [re.sub(r'-', ' ', abstract) for abstract in abstracts]\n",
    "abstracts = [re.sub(r'[^\\w\\s]', '', abstract) for abstract in abstracts]\n",
    "\n",
    "print(abstracts[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to further process each abstract by converting it to lower case, stripping leading and trailing white space, and then tokenizing by splitting on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_tok = [abstract.lower().strip().split(' ') for abstract in abstracts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will further remove tokens that have digits, have possessives or contractions, or are empty strings.\n",
    "\n",
    "### 2.1 Removing special strings\n",
    "\n",
    "Write functions that will achieve that functionality described. Specifically, write\n",
    "- `is_numeric(string)` which checks if `string` has any numbers\n",
    "- `has_poss_contr(string)` which checks if `string` has possessives or contractions\n",
    "- `empty_string(string)` which checks if `string` is an empty string\n",
    "- `remove_string(string)` which checcks if `string` should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(string):\n",
    "    return bool(re.search('\\d', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_poss_contr(string):\n",
    "    blacklist_in = [\"'s\", \"'m\", \"'re\", \"'ve\", \"'d\", \"'ll\", \"'t\"]\n",
    "    return any(i in string for i in blacklist_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_string(string):\n",
    "    return not bool(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string(string):\n",
    "    return is_numeric(string) or has_poss_contr(string) or empty_string(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_tok = [[token for token in abstract if not remove_string(token)] for abstract in abstracts_tok]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling is a collection of powerful statistical methods for discovering abstract \"topics\" that occur in a collection of documents. The overall intuition behind topic models is that each document is only about a few topics, and similar documents contain similar words. \n",
    "\n",
    "For example, the two documents below are similar because over the entire vocabulary of words, they contain similar frequently appearing words like \"complex\", \"network\", \"epidemic\".\n",
    "\n",
    "```\n",
    "{\n",
    "   \"1407.0001\": \"The topic of finding effective strategy to halt virus in complex network is of current interest. We propose an immunization strategy for seasonal epidemics that occur periodically. Based on the local information of the infection status from the previous epidemic season, the selection of vaccinated nodes is optimized gradually. The evolution of vaccinated nodes during iterations demonstrates that the immunization tends to locate in both global hubs and local hubs. We analyze the epidemic prevalence by a heterogeneous mean-field method and present numerical simulations of our model. This immunization performs superiorly to some other previously known strategies. Our work points out a new direction in immunization of seasonal epidemics.\", \n",
    "```\n",
    "\n",
    "```\n",
    "  \"1407.0774\": \"The spread of disease on complex networks has attracted widely attention in the physics community. Recent works have demonstrated that heterogeneous degree and weight distributions have a significant influence on the epidemic dynamics. In this study, a novel edge-weight based compartmental approach is developed to estimate the epidemic threshold and epidemic size (final infected density) on networks with general degree and weight distributions, and a remarkable agreement with numerics is obtained. Even in complex network with the strong heterogeneous degree and weight distributions, this approach is worked. We then propose an edge-weight based removal strategy with different biases, and find that such a strategy can effectively control the spread of epidemic when the highly weighted edges are preferentially removed, especially when the weight distribution of a network is extremely heterogenous. The theoretical results from the suggested method can accurately predict the above removal effectiveness.\", \n",
    "```\n",
    "\n",
    "To build topic models, we require the following components:\n",
    "- A *vocabulary of tokens* that appear across all documents.\n",
    "- A *mapping of tokens to a unique integer identifier*, because topic model algorithms treat words by these identifiers, and not by the strings themselves. For example, we represent `'epidemic'` as `word2id['epidemic'] = 50`\n",
    "- A *corpus*, where each document in the corpus is a collection of tokens, where each token is represented by the identifier and the number of times it appears in the document. For example, in the first document above the token `'epidemic'`, which appears twice, is represented as `(50, 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will build a vocabulary representing the tokens that have appeared across all the abstracts we have. When doing this, we typically want to (1) remove rare words, (2) remove stop words and (3) stem/lemmatize words (which we will not do).\n",
    "\n",
    "To do this, we can use the `Counter` class. The `Counter` is an extension of the Python dictionary which is a set of key-value pairs. For the `Counter` class, keys are the objects to be counted, while values are their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 75970\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "for abstract in abstracts_tok:\n",
    "    vocab.update(abstract)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing rare words helps prevent our vocabulary from being too large. Many tokens appear only a few times across all the abstracts. Keeping them in the vocabulary increases subsequent computation time. Furthermore, their presence tends not to carry much significance for a document, since they can be considered as anomalies.\n",
    "\n",
    "We remove rare words by only keeping tokens that appear more than 25 times across all abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 10480\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(token for token in vocab.elements() if vocab[token] > 25)\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are defined as very common words such as `'the'` and `'a'`. Removing stop words is important because their presence also does not carry much significance, since they appear in all kinds of texts.\n",
    "\n",
    "We will remove stop words by removing the 200 most common tokens across all abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 10280\n"
     ]
    }
   ],
   "source": [
    "stop_words = [item[0] for item in vocab.most_common(200)]\n",
    "vocab = Counter(token for token in vocab.elements() if token not in stop_words)\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a mapping for tokens to unique identifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens mapped: 10280\n",
      "Identifier for 'epidemic': 15\n",
      "Word for identifier 15: epidemic\n"
     ]
    }
   ],
   "source": [
    "id2word = {idx: pair[0] for idx, pair in enumerate(vocab.items())}\n",
    "word2id = {pair[0]: idx for idx, pair in enumerate(vocab.items())}\n",
    "\n",
    "print(\"Number of tokens mapped: %d\" % len(id2word))\n",
    "print(\"Identifier for 'epidemic': %d\" % word2id['epidemic'])\n",
    "print(\"Word for identifier %d: %s\" % (word2id['epidemic'], id2word[word2id['epidemic']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will remove, for each abstract, the tokens that are not found in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_tok = [[token for token in abstract if token in vocab] for abstract in abstracts_tok]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the corpus. Recall that for the tokens in an abstract:\n",
    "\n",
    "```\n",
    "['statistical', 'inconsistency', 'binomial', 'likelihood', 'count', 'identified.', 'issue', 'occurs', 'when', 'response,', 'zero', 'statistically', 'inconsistent', 'erroneous', 'parameter', 'inferences', 'being', 'drawn', 'data.', 'binomial', 'likelihood', 'address', 'issue', 'resulting', 'fully', 'symmetric', 'binomial', 'likelihood', 'zero', 'counts.', 'present', 'simple', 'regression', 'example', 'ecological', 'literature', 'details', 'practical', 'application', 'likelihood', 'model.']\n",
    "```\n",
    "the corpus has the format\n",
    "```\n",
    "[(1396, 1), (1397, 1), (1398, 3), (1399, 4), (1400, 1), (1401, 1), (1402, 2), (1403, 1), (591, 1), (672, 1), (34, 2), (1404, 2), (1405, 1), (1406, 1), (1407, 1), (444, 1), (1408, 1), (1409, 1), (1410, 1), (1187, 1), (610, 1), (1411, 1), (857, 1), (1412, 1), (41, 1), (554, 1), (1413, 1), (1414, 1), (1415, 1), (1416, 1), (1205, 1), (91, 1), (945, 1)]\n",
    "```\n",
    "\n",
    "where each element is a pair containing the identifier for the token and the count of that token in just that abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract, tokenized:\n",
      " ['statistical', 'inconsistency', 'zero', 'inflated', 'binomial', 'likelihood', 'count', 'identified', 'issue', 'occurs', 'response', 'zero', 'n', 'inflated', 'statistically', 'inconsistent', 'erroneous', 'inferences', 'being', 'drawn', 'zero', 'modified', 'binomial', 'likelihood', 'address', 'issue', 'n', 'inflation', 'resulting', 'fully', 'symmetric', 'binomial', 'likelihood', 'zero', 'n', 'inflated', 'counts', 'regression', 'example', 'ecological', 'literature', 'details', 'practical', 'application', 'likelihood'] \n",
      "\n",
      "Abstract, in corpus format:\n",
      " [(1204, 1), (1205, 1), (1206, 4), (1207, 3), (1208, 3), (1209, 4), (1210, 1), (1211, 1), (1212, 2), (1213, 1), (555, 1), (1131, 3), (1214, 1), (1215, 1), (1216, 1), (1217, 1), (1218, 1), (1219, 1), (464, 1), (1017, 1), (1220, 1), (505, 1), (1221, 1), (722, 1), (1222, 1), (1223, 1), (1224, 1), (1225, 1), (1226, 1), (1034, 1), (68, 1), (797, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for abstract in abstracts_tok:\n",
    "    abstract_count = Counter(abstract)\n",
    "    corpus.append([(word2id[item[0]], item[1]) for item in abstract_count.items()])\n",
    "\n",
    "print(\"Abstract, tokenized:\\n\", abstracts_tok[sample], \"\\n\")\n",
    "print(\"Abstract, in corpus format:\\n\", corpus[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a topic model\n",
    "\n",
    "Now, we are ready to create our topic model!\n",
    "\n",
    "We will use gensim, a Python library to create topic models. We will use the algorithm called latent dirichlet allocation implemented in the gensim library. \n",
    "\n",
    "**This step takes some time (about 4 min)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the topic model, we want to view the 10 topics. The topics are represented as a combination of keywords with corresponding weight on the keyword. Note that the order of these topics can change between different training runs of the topic model, since there is no ordering between topics and gensim returns them in an arbitrary order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word rank</th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bound</td>\n",
       "      <td>boundary</td>\n",
       "      <td>spectral</td>\n",
       "      <td>classical</td>\n",
       "      <td>light</td>\n",
       "      <td>flow</td>\n",
       "      <td>multiple</td>\n",
       "      <td>interaction</td>\n",
       "      <td>algorithms</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>matter</td>\n",
       "      <td>approximation</td>\n",
       "      <td>spectrum</td>\n",
       "      <td>cluster</td>\n",
       "      <td>frequency</td>\n",
       "      <td>points</td>\n",
       "      <td>noise</td>\n",
       "      <td>interactions</td>\n",
       "      <td>neural</td>\n",
       "      <td>ray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>dark</td>\n",
       "      <td>graphs</td>\n",
       "      <td>region</td>\n",
       "      <td>out</td>\n",
       "      <td>topological</td>\n",
       "      <td>groups</td>\n",
       "      <td>existing</td>\n",
       "      <td>critical</td>\n",
       "      <td>image</td>\n",
       "      <td>observations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>lower</td>\n",
       "      <td>nonlinear</td>\n",
       "      <td>measurements</td>\n",
       "      <td>stable</td>\n",
       "      <td>induced</td>\n",
       "      <td>dimension</td>\n",
       "      <td>techniques</td>\n",
       "      <td>particle</td>\n",
       "      <td>applications</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>task</td>\n",
       "      <td>existence</td>\n",
       "      <td>line</td>\n",
       "      <td>those</td>\n",
       "      <td>scalar</td>\n",
       "      <td>spaces</td>\n",
       "      <td>multi</td>\n",
       "      <td>symmetry</td>\n",
       "      <td>control</td>\n",
       "      <td>star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>radio</td>\n",
       "      <td>convergence</td>\n",
       "      <td>velocity</td>\n",
       "      <td>similar</td>\n",
       "      <td>gravity</td>\n",
       "      <td>symmetric</td>\n",
       "      <td>scheme</td>\n",
       "      <td>lattice</td>\n",
       "      <td>classification</td>\n",
       "      <td>galaxies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>upper</td>\n",
       "      <td>operator</td>\n",
       "      <td>resolution</td>\n",
       "      <td>so</td>\n",
       "      <td>optical</td>\n",
       "      <td>algebra</td>\n",
       "      <td>communication</td>\n",
       "      <td>electron</td>\n",
       "      <td>novel</td>\n",
       "      <td>emission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>decay</td>\n",
       "      <td>derive</td>\n",
       "      <td>family</td>\n",
       "      <td>fields</td>\n",
       "      <td>modes</td>\n",
       "      <td>theorem</td>\n",
       "      <td>available</td>\n",
       "      <td>scattering</td>\n",
       "      <td>training</td>\n",
       "      <td>sample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>generation</td>\n",
       "      <td>second</td>\n",
       "      <td>ratio</td>\n",
       "      <td>black</td>\n",
       "      <td>physics</td>\n",
       "      <td>metric</td>\n",
       "      <td>action</td>\n",
       "      <td>dependent</td>\n",
       "      <td>optimization</td>\n",
       "      <td>gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>core</td>\n",
       "      <td>positive</td>\n",
       "      <td>stability</td>\n",
       "      <td>initial</td>\n",
       "      <td>measurement</td>\n",
       "      <td>representation</td>\n",
       "      <td>compared</td>\n",
       "      <td>particles</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>formation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>flux</td>\n",
       "      <td>value</td>\n",
       "      <td>background</td>\n",
       "      <td>long</td>\n",
       "      <td>description</td>\n",
       "      <td>condition</td>\n",
       "      <td>distributed</td>\n",
       "      <td>coupled</td>\n",
       "      <td>features</td>\n",
       "      <td>stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>change</td>\n",
       "      <td>probability</td>\n",
       "      <td>cross</td>\n",
       "      <td>motion</td>\n",
       "      <td>mode</td>\n",
       "      <td>complex</td>\n",
       "      <td>code</td>\n",
       "      <td>ground</td>\n",
       "      <td>art</td>\n",
       "      <td>galaxy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>memory</td>\n",
       "      <td>measure</td>\n",
       "      <td>were</td>\n",
       "      <td>dynamical</td>\n",
       "      <td>photon</td>\n",
       "      <td>arbitrary</td>\n",
       "      <td>channel</td>\n",
       "      <td>charge</td>\n",
       "      <td>stochastic</td>\n",
       "      <td>sources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>game</td>\n",
       "      <td>discrete</td>\n",
       "      <td>near</td>\n",
       "      <td>i</td>\n",
       "      <td>vector</td>\n",
       "      <td>invariant</td>\n",
       "      <td>nodes</td>\n",
       "      <td>momentum</td>\n",
       "      <td>images</td>\n",
       "      <td>stellar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>strength</td>\n",
       "      <td>operators</td>\n",
       "      <td>prediction</td>\n",
       "      <td>every</td>\n",
       "      <td>define</td>\n",
       "      <td>defined</td>\n",
       "      <td>achieve</td>\n",
       "      <td>regime</td>\n",
       "      <td>design</td>\n",
       "      <td>massive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word rank     topic 0        topic 1       topic 2    topic 3  \\\n",
       "0           1       bound       boundary      spectral  classical   \n",
       "1           2      matter  approximation      spectrum    cluster   \n",
       "2           3        dark         graphs        region        out   \n",
       "3           4       lower      nonlinear  measurements     stable   \n",
       "4           5        task      existence          line      those   \n",
       "5           6       radio    convergence      velocity    similar   \n",
       "6           7       upper       operator    resolution         so   \n",
       "7           8       decay         derive        family     fields   \n",
       "8           9  generation         second         ratio      black   \n",
       "9          10        core       positive     stability    initial   \n",
       "10         11        flux          value    background       long   \n",
       "11         12      change    probability         cross     motion   \n",
       "12         13      memory        measure          were  dynamical   \n",
       "13         14        game       discrete          near          i   \n",
       "14         15    strength      operators    prediction      every   \n",
       "\n",
       "        topic 4         topic 5        topic 6       topic 7         topic 8  \\\n",
       "0         light            flow       multiple   interaction      algorithms   \n",
       "1     frequency          points          noise  interactions          neural   \n",
       "2   topological          groups       existing      critical           image   \n",
       "3       induced       dimension     techniques      particle    applications   \n",
       "4        scalar          spaces          multi      symmetry         control   \n",
       "5       gravity       symmetric         scheme       lattice  classification   \n",
       "6       optical         algebra  communication      electron           novel   \n",
       "7         modes         theorem      available    scattering        training   \n",
       "8       physics          metric         action     dependent    optimization   \n",
       "9   measurement  representation       compared     particles        accuracy   \n",
       "10  description       condition    distributed       coupled        features   \n",
       "11         mode         complex           code        ground             art   \n",
       "12       photon       arbitrary        channel        charge      stochastic   \n",
       "13       vector       invariant          nodes      momentum          images   \n",
       "14       define         defined        achieve        regime          design   \n",
       "\n",
       "         topic 9  \n",
       "0           deep  \n",
       "1            ray  \n",
       "2   observations  \n",
       "3              x  \n",
       "4           star  \n",
       "5       galaxies  \n",
       "6       emission  \n",
       "7         sample  \n",
       "8            gas  \n",
       "9      formation  \n",
       "10         stars  \n",
       "11        galaxy  \n",
       "12       sources  \n",
       "13       stellar  \n",
       "14       massive  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_topics = 10\n",
    "num_words = 15\n",
    "top_words = pd.DataFrame({'word rank': np.arange(1,num_words+1)})\n",
    "for k in np.arange(num_topics): \n",
    "    topic = lda_model.get_topic_terms(k, num_words)\n",
    "    words = [id2word[topic[i][0]] for i in np.arange(num_words)]\n",
    "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
    "    top_words['topic %d' % k] = words\n",
    "\n",
    "top_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the probability distribution for a given abstract in the `corpus`. This represents how likely it is for the abstract to belong to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.05851045),\n",
       " (1, 0.07150707),\n",
       " (2, 0.019132039),\n",
       " (3, 0.046507437),\n",
       " (4, 0.02102072),\n",
       " (6, 0.011875552),\n",
       " (7, 0.076684006),\n",
       " (8, 0.061220784),\n",
       " (9, 0.6250374)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 10\n",
    "lda_model.get_document_topics(corpus[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's represent this as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.019140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.046509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.011861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.076686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.061221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.625040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Probabilities\n",
       "0      0       0.058510\n",
       "1      1       0.071506\n",
       "2      2       0.019140\n",
       "3      3       0.046509\n",
       "4      4       0.021021\n",
       "5      6       0.011861\n",
       "6      7       0.076686\n",
       "7      8       0.061221\n",
       "8      9       0.625040"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_dist = lda_model.get_document_topics(corpus[sample])\n",
    "topics = [pair[0] for pair in topic_dist]\n",
    "probabilities = [pair[1] for pair in topic_dist]\n",
    "topic_dist_table = pd.DataFrame()\n",
    "topic_dist_table['Topic'] = topics\n",
    "topic_dist_table['Probabilities'] = probabilities\n",
    "topic_dist_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic with highest probability: 8 (0.625040)\n"
     ]
    }
   ],
   "source": [
    "t = np.argmax(probabilities)\n",
    "print(\"Topic with highest probability: %d (%f)\" % (t, probabilities[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(11,4)\n",
    "plt.bar(topic_dist_table['Topic'], topic_dist_table['Probabilities'], align='center', alpha=1, color='salmon')\n",
    "plt.xlabel('topic')\n",
    "plt.ylabel('probability')\n",
    "plt.title('Per Topic Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems the document\n",
    "\n",
    "```\n",
    "  \"1407.0034\": \"We use cosmological simulations to assess how the explosion of the first stars in supernovae (SNe) influences early cosmic history. Specifically, we investigate the impact by SNe on the host systems for Population~III (Pop~III) star formation and explore its dependence on halo environment and Pop~III progenitor mass. We then trace the evolution of the enriched gas until conditions are met to trigger second-generation star formation. To this extent, we quantify the recovery timescale, which measures the time delay between a Pop~III SN explosion and the appearance of cold, dense gas, out of which second-generation stars can form. We find that this timescale is highly sensitive to the Pop~III progenitor mass, and less so to the halo environment. For Pop~III progenitor masses, $M_{\\\\ast}=15{{\\\\,M_\\\\odot}}$, $25{{\\\\,M_\\\\odot}}$, and $40{{\\\\,M_\\\\odot}}$ in a halo of $5\\\\times10^5{{\\\\,M_\\\\odot}}$, recovery times are $\\\\sim10$\\\\,Myr, $25$\\\\,Myr, and $90$\\\\,Myr, respectively. For more massive progenitors, including those exploding in pair instability SNe, second-generation star formation is delayed significantly, for up to a Hubble time. The dependence of the recovery time on the mass of the SN progenitor is mainly due to the ionizing impact of the progenitor star. Photoionization heating increases the gas pressure and initiates a hydrodynamical response that reduces the central gas density, an effect that is stronger in more massive. The gas around lower mass Pop~III stars remains denser and hence the SN remnants cool more rapidly, facilitating the subsequent re-condensation of the gas and formation of a second generation of stars. In most cases, the second-generation stars are already metal-enriched to $\\\\sim2-5\\\\times10^{-4}{{\\\\rm \\\\,Z_\\\\odot}}$, thus belonging to Population~II. The recovery timescale is a key quantity governing the nature of the first galaxies, able to host low-mass, long-lived stellar systems. These in turn are the target of future deep-field campaigns with the {\\\\it James Webb Space Telescope}.\", \n",
    "```\n",
    "\n",
    "has the greatest likelihood to fall under the topic number with topic relating to cosmology, which matches our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics for Abstracts\n",
    "\n",
    "*Your turn!* Your task is broken down into three parts\n",
    "\n",
    "#### 2.2 Label the Topics\n",
    "Label all the 10 topics with your interpretation of what the topics are. This may not come naturally since we are dealing with scientific articles, but do your best! Incorporate the labels by changing the names of the columns in the `top_words` table above. Also form a new table `labels` with 2 columns, where the column names are `topic_num` and `topic_label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapTopicsToLabels = {\n",
    "    0 : \"dark matter\", \n",
    "    1 : \"graph theory\",\n",
    "    2 : \"light measurment\", \n",
    "    3 : \"classical physics\", \n",
    "    4 : \"black holes\",\n",
    "    5 : \"maximum flow problem\", \n",
    "    6 : \"noisy communication\",\n",
    "    7 : \"wave-particle interaction\",\n",
    "    8 : \"image classification\",\n",
    "    9 : \"cosmology\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word rank</th>\n",
       "      <th>dark matter</th>\n",
       "      <th>graph theory</th>\n",
       "      <th>light measurment</th>\n",
       "      <th>classical physics</th>\n",
       "      <th>black holes</th>\n",
       "      <th>maximum flow problem</th>\n",
       "      <th>noisy communication</th>\n",
       "      <th>wave-particle interaction</th>\n",
       "      <th>image classification</th>\n",
       "      <th>cosmology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bound</td>\n",
       "      <td>boundary</td>\n",
       "      <td>spectral</td>\n",
       "      <td>classical</td>\n",
       "      <td>light</td>\n",
       "      <td>flow</td>\n",
       "      <td>multiple</td>\n",
       "      <td>interaction</td>\n",
       "      <td>algorithms</td>\n",
       "      <td>deep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>matter</td>\n",
       "      <td>approximation</td>\n",
       "      <td>spectrum</td>\n",
       "      <td>cluster</td>\n",
       "      <td>frequency</td>\n",
       "      <td>points</td>\n",
       "      <td>noise</td>\n",
       "      <td>interactions</td>\n",
       "      <td>neural</td>\n",
       "      <td>ray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>dark</td>\n",
       "      <td>graphs</td>\n",
       "      <td>region</td>\n",
       "      <td>out</td>\n",
       "      <td>topological</td>\n",
       "      <td>groups</td>\n",
       "      <td>existing</td>\n",
       "      <td>critical</td>\n",
       "      <td>image</td>\n",
       "      <td>observations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>lower</td>\n",
       "      <td>nonlinear</td>\n",
       "      <td>measurements</td>\n",
       "      <td>stable</td>\n",
       "      <td>induced</td>\n",
       "      <td>dimension</td>\n",
       "      <td>techniques</td>\n",
       "      <td>particle</td>\n",
       "      <td>applications</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>task</td>\n",
       "      <td>existence</td>\n",
       "      <td>line</td>\n",
       "      <td>those</td>\n",
       "      <td>scalar</td>\n",
       "      <td>spaces</td>\n",
       "      <td>multi</td>\n",
       "      <td>symmetry</td>\n",
       "      <td>control</td>\n",
       "      <td>star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>radio</td>\n",
       "      <td>convergence</td>\n",
       "      <td>velocity</td>\n",
       "      <td>similar</td>\n",
       "      <td>gravity</td>\n",
       "      <td>symmetric</td>\n",
       "      <td>scheme</td>\n",
       "      <td>lattice</td>\n",
       "      <td>classification</td>\n",
       "      <td>galaxies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>upper</td>\n",
       "      <td>operator</td>\n",
       "      <td>resolution</td>\n",
       "      <td>so</td>\n",
       "      <td>optical</td>\n",
       "      <td>algebra</td>\n",
       "      <td>communication</td>\n",
       "      <td>electron</td>\n",
       "      <td>novel</td>\n",
       "      <td>emission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>decay</td>\n",
       "      <td>derive</td>\n",
       "      <td>family</td>\n",
       "      <td>fields</td>\n",
       "      <td>modes</td>\n",
       "      <td>theorem</td>\n",
       "      <td>available</td>\n",
       "      <td>scattering</td>\n",
       "      <td>training</td>\n",
       "      <td>sample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>generation</td>\n",
       "      <td>second</td>\n",
       "      <td>ratio</td>\n",
       "      <td>black</td>\n",
       "      <td>physics</td>\n",
       "      <td>metric</td>\n",
       "      <td>action</td>\n",
       "      <td>dependent</td>\n",
       "      <td>optimization</td>\n",
       "      <td>gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>core</td>\n",
       "      <td>positive</td>\n",
       "      <td>stability</td>\n",
       "      <td>initial</td>\n",
       "      <td>measurement</td>\n",
       "      <td>representation</td>\n",
       "      <td>compared</td>\n",
       "      <td>particles</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>formation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>flux</td>\n",
       "      <td>value</td>\n",
       "      <td>background</td>\n",
       "      <td>long</td>\n",
       "      <td>description</td>\n",
       "      <td>condition</td>\n",
       "      <td>distributed</td>\n",
       "      <td>coupled</td>\n",
       "      <td>features</td>\n",
       "      <td>stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>change</td>\n",
       "      <td>probability</td>\n",
       "      <td>cross</td>\n",
       "      <td>motion</td>\n",
       "      <td>mode</td>\n",
       "      <td>complex</td>\n",
       "      <td>code</td>\n",
       "      <td>ground</td>\n",
       "      <td>art</td>\n",
       "      <td>galaxy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>memory</td>\n",
       "      <td>measure</td>\n",
       "      <td>were</td>\n",
       "      <td>dynamical</td>\n",
       "      <td>photon</td>\n",
       "      <td>arbitrary</td>\n",
       "      <td>channel</td>\n",
       "      <td>charge</td>\n",
       "      <td>stochastic</td>\n",
       "      <td>sources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>game</td>\n",
       "      <td>discrete</td>\n",
       "      <td>near</td>\n",
       "      <td>i</td>\n",
       "      <td>vector</td>\n",
       "      <td>invariant</td>\n",
       "      <td>nodes</td>\n",
       "      <td>momentum</td>\n",
       "      <td>images</td>\n",
       "      <td>stellar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>strength</td>\n",
       "      <td>operators</td>\n",
       "      <td>prediction</td>\n",
       "      <td>every</td>\n",
       "      <td>define</td>\n",
       "      <td>defined</td>\n",
       "      <td>achieve</td>\n",
       "      <td>regime</td>\n",
       "      <td>design</td>\n",
       "      <td>massive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word rank dark matter   graph theory light measurment classical physics  \\\n",
       "0           1       bound       boundary         spectral         classical   \n",
       "1           2      matter  approximation         spectrum           cluster   \n",
       "2           3        dark         graphs           region               out   \n",
       "3           4       lower      nonlinear     measurements            stable   \n",
       "4           5        task      existence             line             those   \n",
       "5           6       radio    convergence         velocity           similar   \n",
       "6           7       upper       operator       resolution                so   \n",
       "7           8       decay         derive           family            fields   \n",
       "8           9  generation         second            ratio             black   \n",
       "9          10        core       positive        stability           initial   \n",
       "10         11        flux          value       background              long   \n",
       "11         12      change    probability            cross            motion   \n",
       "12         13      memory        measure             were         dynamical   \n",
       "13         14        game       discrete             near                 i   \n",
       "14         15    strength      operators       prediction             every   \n",
       "\n",
       "    black holes maximum flow problem noisy communication  \\\n",
       "0         light                 flow            multiple   \n",
       "1     frequency               points               noise   \n",
       "2   topological               groups            existing   \n",
       "3       induced            dimension          techniques   \n",
       "4        scalar               spaces               multi   \n",
       "5       gravity            symmetric              scheme   \n",
       "6       optical              algebra       communication   \n",
       "7         modes              theorem           available   \n",
       "8       physics               metric              action   \n",
       "9   measurement       representation            compared   \n",
       "10  description            condition         distributed   \n",
       "11         mode              complex                code   \n",
       "12       photon            arbitrary             channel   \n",
       "13       vector            invariant               nodes   \n",
       "14       define              defined             achieve   \n",
       "\n",
       "   wave-particle interaction image classification     cosmology  \n",
       "0                interaction           algorithms          deep  \n",
       "1               interactions               neural           ray  \n",
       "2                   critical                image  observations  \n",
       "3                   particle         applications             x  \n",
       "4                   symmetry              control          star  \n",
       "5                    lattice       classification      galaxies  \n",
       "6                   electron                novel      emission  \n",
       "7                 scattering             training        sample  \n",
       "8                  dependent         optimization           gas  \n",
       "9                  particles             accuracy     formation  \n",
       "10                   coupled             features         stars  \n",
       "11                    ground                  art        galaxy  \n",
       "12                    charge           stochastic       sources  \n",
       "13                  momentum               images       stellar  \n",
       "14                    regime               design       massive  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_names = [\"word rank\"] + [v for v in mapTopicsToLabels.values()]\n",
    "top_words.columns=new_names\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(\n",
    "    zip(mapTopicsToLabels.keys(), mapTopicsToLabels.values()),\n",
    "    columns=[\"topic_num\", \"topic_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dark matter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>graph theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>light measurment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>classical physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>black holes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>maximum flow problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>noisy communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>wave-particle interaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>image classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>cosmology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_num                topic_label\n",
       "0          0                dark matter\n",
       "1          1               graph theory\n",
       "2          2           light measurment\n",
       "3          3          classical physics\n",
       "4          4                black holes\n",
       "5          5       maximum flow problem\n",
       "6          6        noisy communication\n",
       "7          7  wave-particle interaction\n",
       "8          8       image classification\n",
       "9          9                  cosmology"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Table of Topics for Abstracts\n",
    "Create a function `create_topic_table(data, abstracts, corpus, lda_model)` which does the following:\n",
    "- Goes through every abstract in the word-index form `corpus`, finding the most likely topic for that abstract\n",
    "- Creates a table `topic_table` that has the following columns\n",
    "    - `arxiv id`: the arXiv document number of each abstract\n",
    "    - `topic`: the topic number of the most likely topic for each abstract\n",
    "    - `label`: the topic label of that topic number, which you assigned in part 1\n",
    "    - `prob`: the probability of that topic number for each abstract\n",
    "    - `abstract`: a string containing the first 200 characters of the abstract\n",
    "- Show the first 10 rows of the table, then return the table\n",
    "\n",
    "You can refer to the function skeleton below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_topic_table(data, abstracts, corpus, lda_model):\n",
    "    \n",
    "    # retrieve the arXiv ids by iterating over data.items()\n",
    "    \n",
    "    # retrieve the topic numbers, labels and probabilities\n",
    "    # for each abstract in the corpus, \n",
    "    # - find the topic distribution for the abstract\n",
    "    # - get the topic number with highest probability,\n",
    "    # - get the topic label associated with tthee topic number\n",
    "    # - get the label for that topic\n",
    "    \n",
    "    # create table with\n",
    "    # - column of arXiv ids\n",
    "    # - column of topic numbers\n",
    "    # - column of topic labels\n",
    "    # - columnn of topic probabilities of that topic\n",
    "    # - column of text for the abstract\n",
    "    \n",
    "    # show first 10 rows of the table \n",
    "    # return the table\n",
    "    \n",
    "    # Here is some sample code you could use to get started:\n",
    "    arxiv_id = [item[0] for item in data.items()]\n",
    "    \n",
    "    \n",
    "    # initialize some arrays\n",
    "    probs = []\n",
    "    topic = []\n",
    "    label = []\n",
    "    short_abstract = []\n",
    "    for sample in np.arange(len(corpus)):\n",
    "        short_abstract.append(abstracts[sample][0:200])\n",
    "        \n",
    "        # Now find the most probable topic by calling \n",
    "        topic_dist = lda_model.get_document_topics(corpus[sample])\n",
    "        _topics = [pair[0] for pair in topic_dist]\n",
    "        _probabilities = [pair[1] for pair in topic_dist]\n",
    "        \n",
    "        # Try using np.argmax\n",
    "        _t = np.argmax(_probabilities)\n",
    "        probs.append(_probabilities[_t])\n",
    "        topic.append(_topics[_t])\n",
    "        label.append(mapTopicsToLabels[_topics[_t]])\n",
    "        \n",
    "    table = pd.DataFrame()\n",
    "    table['arxiv id'] = arxiv_id # attach the entire column\n",
    "    table['abstract'] = short_abstract # attach the entire column\n",
    "    table['topic'] = topic\n",
    "    table['label'] = label\n",
    "    table['prob'] = probs\n",
    "    # You'll need to add the topic, label, and probability for each abstract\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1407.0001</td>\n",
       "      <td>The topic of finding effective strategy to hal...</td>\n",
       "      <td>9</td>\n",
       "      <td>cosmology</td>\n",
       "      <td>0.323206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1407.0004</td>\n",
       "      <td>Linear precoding exploits the spatial degrees ...</td>\n",
       "      <td>6</td>\n",
       "      <td>noisy communication</td>\n",
       "      <td>0.499424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1407.0016</td>\n",
       "      <td>We present completed observations of the NGC 7...</td>\n",
       "      <td>9</td>\n",
       "      <td>cosmology</td>\n",
       "      <td>0.444528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1407.0017</td>\n",
       "      <td>Increasingly stringent limits from LHC searche...</td>\n",
       "      <td>0</td>\n",
       "      <td>dark matter</td>\n",
       "      <td>0.224427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1407.0023</td>\n",
       "      <td>Results are presented for an initial survey of...</td>\n",
       "      <td>9</td>\n",
       "      <td>cosmology</td>\n",
       "      <td>0.397316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1407.0026</td>\n",
       "      <td>Current time domain wide field sky surveys gen...</td>\n",
       "      <td>9</td>\n",
       "      <td>cosmology</td>\n",
       "      <td>0.522321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1407.0029</td>\n",
       "      <td>Spinless fermions on a honeycomb lattice provi...</td>\n",
       "      <td>7</td>\n",
       "      <td>wave-particle interaction</td>\n",
       "      <td>0.614909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1407.0030</td>\n",
       "      <td>3pt  The strong CP violating parameter is sma...</td>\n",
       "      <td>4</td>\n",
       "      <td>black holes</td>\n",
       "      <td>0.588786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1407.0031</td>\n",
       "      <td>Cross correlating the Planck High Frequency In...</td>\n",
       "      <td>9</td>\n",
       "      <td>cosmology</td>\n",
       "      <td>0.408977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1407.0033</td>\n",
       "      <td>We study low energy effective field theories f...</td>\n",
       "      <td>7</td>\n",
       "      <td>wave-particle interaction</td>\n",
       "      <td>0.272530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1407.0034</td>\n",
       "      <td>We use cosmological simulations to assess how ...</td>\n",
       "      <td>9</td>\n",
       "      <td>cosmology</td>\n",
       "      <td>0.625037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1407.0035</td>\n",
       "      <td>This paper describes the advantages and challe...</td>\n",
       "      <td>7</td>\n",
       "      <td>wave-particle interaction</td>\n",
       "      <td>0.305037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1407.0036</td>\n",
       "      <td>Slow parameter drift is common in many systems...</td>\n",
       "      <td>8</td>\n",
       "      <td>image classification</td>\n",
       "      <td>0.265753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1407.0037</td>\n",
       "      <td>Actively shought since the turn of the centur...</td>\n",
       "      <td>7</td>\n",
       "      <td>wave-particle interaction</td>\n",
       "      <td>0.581974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1407.0038</td>\n",
       "      <td>We continue our exploration of the nearly Pecc...</td>\n",
       "      <td>0</td>\n",
       "      <td>dark matter</td>\n",
       "      <td>0.417251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1407.0039</td>\n",
       "      <td>The following  document accompanies the papers...</td>\n",
       "      <td>9</td>\n",
       "      <td>cosmology</td>\n",
       "      <td>0.405932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1407.0040</td>\n",
       "      <td>The contact interaction is often used in model...</td>\n",
       "      <td>7</td>\n",
       "      <td>wave-particle interaction</td>\n",
       "      <td>0.294946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1407.0041</td>\n",
       "      <td>The paper describes an explicit variational mo...</td>\n",
       "      <td>7</td>\n",
       "      <td>wave-particle interaction</td>\n",
       "      <td>0.347419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1407.0042</td>\n",
       "      <td>Where the doped holes reside in cuprate superc...</td>\n",
       "      <td>3</td>\n",
       "      <td>classical physics</td>\n",
       "      <td>0.466394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1407.0043</td>\n",
       "      <td>A subgraph in an  edge colored graph is  multi...</td>\n",
       "      <td>8</td>\n",
       "      <td>image classification</td>\n",
       "      <td>0.710604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     arxiv id                                           abstract  topic  \\\n",
       "0   1407.0001  The topic of finding effective strategy to hal...      9   \n",
       "1   1407.0004  Linear precoding exploits the spatial degrees ...      6   \n",
       "2   1407.0016  We present completed observations of the NGC 7...      9   \n",
       "3   1407.0017  Increasingly stringent limits from LHC searche...      0   \n",
       "4   1407.0023  Results are presented for an initial survey of...      9   \n",
       "5   1407.0026  Current time domain wide field sky surveys gen...      9   \n",
       "6   1407.0029  Spinless fermions on a honeycomb lattice provi...      7   \n",
       "7   1407.0030   3pt  The strong CP violating parameter is sma...      4   \n",
       "8   1407.0031  Cross correlating the Planck High Frequency In...      9   \n",
       "9   1407.0033  We study low energy effective field theories f...      7   \n",
       "10  1407.0034  We use cosmological simulations to assess how ...      9   \n",
       "11  1407.0035  This paper describes the advantages and challe...      7   \n",
       "12  1407.0036  Slow parameter drift is common in many systems...      8   \n",
       "13  1407.0037   Actively shought since the turn of the centur...      7   \n",
       "14  1407.0038  We continue our exploration of the nearly Pecc...      0   \n",
       "15  1407.0039  The following  document accompanies the papers...      9   \n",
       "16  1407.0040  The contact interaction is often used in model...      7   \n",
       "17  1407.0041  The paper describes an explicit variational mo...      7   \n",
       "18  1407.0042  Where the doped holes reside in cuprate superc...      3   \n",
       "19  1407.0043  A subgraph in an  edge colored graph is  multi...      8   \n",
       "\n",
       "                        label      prob  \n",
       "0                   cosmology  0.323206  \n",
       "1         noisy communication  0.499424  \n",
       "2                   cosmology  0.444528  \n",
       "3                 dark matter  0.224427  \n",
       "4                   cosmology  0.397316  \n",
       "5                   cosmology  0.522321  \n",
       "6   wave-particle interaction  0.614909  \n",
       "7                 black holes  0.588786  \n",
       "8                   cosmology  0.408977  \n",
       "9   wave-particle interaction  0.272530  \n",
       "10                  cosmology  0.625037  \n",
       "11  wave-particle interaction  0.305037  \n",
       "12       image classification  0.265753  \n",
       "13  wave-particle interaction  0.581974  \n",
       "14                dark matter  0.417251  \n",
       "15                  cosmology  0.405932  \n",
       "16  wave-particle interaction  0.294946  \n",
       "17  wave-particle interaction  0.347419  \n",
       "18          classical physics  0.466394  \n",
       "19       image classification  0.710604  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_table = create_topic_table(data, abstracts, corpus, lda_model)\n",
    "topic_table.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Analysis for selected abstracts\n",
    "Choose at least five articles and discuss how the assignment of topics either does or does not make sense, according to your own understanding of the article.\n",
    "Include your comments in a Markdown cell, with code cells added as needed to pull out particular rows of your table. Note that you can see the original scientific article a given `<arxiv id>` by visiting the web site `http://arxiv.org/abs/<arxiv id>`.  For example, [http://arxiv.org/abs/1407.0001](http://arxiv.org/abs/1407.0001) shows the first paper in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1407.0064</td>\n",
       "      <td>A statistical inconsistency of a zero inflate...</td>\n",
       "      <td>1</td>\n",
       "      <td>graph theory</td>\n",
       "      <td>0.296635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1407.0065</td>\n",
       "      <td>We describe a novel approach for generating li...</td>\n",
       "      <td>3</td>\n",
       "      <td>classical physics</td>\n",
       "      <td>0.435041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1407.0071</td>\n",
       "      <td>We propose a quantum mechanical method of dete...</td>\n",
       "      <td>4</td>\n",
       "      <td>black holes</td>\n",
       "      <td>0.409316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1407.0073</td>\n",
       "      <td>We present a femtosecond optical pump probe st...</td>\n",
       "      <td>7</td>\n",
       "      <td>wave-particle interaction</td>\n",
       "      <td>0.710915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1407.0077</td>\n",
       "      <td>The wavefunction method provides us with a use...</td>\n",
       "      <td>7</td>\n",
       "      <td>wave-particle interaction</td>\n",
       "      <td>0.693140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     arxiv id                                           abstract  topic  \\\n",
       "30  1407.0064   A statistical inconsistency of a zero inflate...      1   \n",
       "31  1407.0065  We describe a novel approach for generating li...      3   \n",
       "32  1407.0071  We propose a quantum mechanical method of dete...      4   \n",
       "33  1407.0073  We present a femtosecond optical pump probe st...      7   \n",
       "34  1407.0077  The wavefunction method provides us with a use...      7   \n",
       "\n",
       "                        label      prob  \n",
       "30               graph theory  0.296635  \n",
       "31          classical physics  0.435041  \n",
       "32                black holes  0.409316  \n",
       "33  wave-particle interaction  0.710915  \n",
       "34  wave-particle interaction  0.693140  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i chose a random selection of 5\n",
    "topic_table.iloc[30:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 30: it is actually about statistical methodology, which is quite close to what I had in mind when I wrote graph theory. In other words, it focuses on the theoretical mathematics, rather than any empirical research.\n",
    "- 31: it is actually a subbranch of physics called optics\n",
    "- 32: it is about quantum physics, not black holes\n",
    "- 33: it is about correlated electrons, which is a very arcane subject, but ostensibly related to wave-particles\n",
    "- 34: it is the same as 33\n",
    "\n",
    "In summary, all topics are quite close, but describe another underlying facet of the data which would have been more clear to me if I was not a layperson and knew some more physics to describe the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Extra credit: Improve the model\n",
    "For extra credit, improve the topic model by improving the processing of the data and the vocabulary, and selecting a more appropriate number of topics. Describe how your new model gives an improvement over the \"quick and dirty\" topic model built above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
