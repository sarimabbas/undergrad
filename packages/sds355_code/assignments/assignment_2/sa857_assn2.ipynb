{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from perspective import psp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice: Unfortunately, due to sickness and multiple commitments, I fell behind on my PSET this week.**\n",
    "\n",
    "I have completed Problem 1 and parts of Problem 3. Thank you for your understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Due: 26th Sept, 11:59pm\n",
    "\n",
    "# Problem 1: Spam, wonderful spam!\n",
    "\n",
    "The dataset consists of a collection of 57 features relating to about 4600 emails and a label of whether or not the email is considered spam. You have a training set containing about 70% of the data and a test set containing about 30% of the data. Your job is to build effective spam classification rules using the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note about Features\n",
    "\n",
    "The column names (in the first row of each .csv file) are fairly self-explanatory.\n",
    "\n",
    "* Some variables are named `word_freq_(word)`, which suggests a calculation of the frequency of how many times a specific word appears in the email, expressed as a percentage of total words in the email multiplied by 100.\n",
    "\n",
    "* Some variables are named `char_freq_(number)`, which suggests a count of the frequency of the specific ensuing character, expressed as a percentage of total characters in the email multiplied by 100. Note, these characters are not valid column names in R, but you can view them in the raw .csv file. \n",
    "\n",
    "* Some variables are named `capital_run_length_(number)` which suggests some information about the average (or maximum length of, or total) consecutive capital letters in the email.\n",
    "\n",
    "* `spam`: This is the response variable, 0 = not spam, 1 = spam.\n",
    "\n",
    "### Missing Values\n",
    "\n",
    "Unfortunately, the `capital_run_length_average` variable is corrupted and as a result, contains a fair number of missing values. These show up as `NaN` (the default way of representing missing values in Python.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a\n",
    "\n",
    "Use $k$-nearest neighbors regression with $k=15$ to **impute** the missing values in the `capital_run_length_average` column using the other predictors after standardizing (i.e. rescaling) them. You may use a function such as <i>KNeighborsRegressor</i>  from the package <i>sklearn.neighbors</i> that performs $k$-nearest neighbors regression. There is no penalty for using a built-in function.\n",
    "\n",
    "When you are done with this part, you should have no more NaN's in the `capital_run_length_average` column in either the training or the test set. To keep the training and test sets separate, you will need to build two models for imputing: one that is trained on, and imputes for, the training set, and another that is trained on, and imputes for, the test set. Make sure you show all of your work. (You may find the function <i>np.isnan()</i> useful for this problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my notes\n",
    "# regression model: codomain of model is a continuous space, e.g. ‚Ñù\n",
    "# classification model: codomain of model is a discrete space, e.g. {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"spam_train.csv\")\n",
    "test = pd.read_csv(\"spam_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleAllColumnsExceptCRLA(df, sc, fit_type):\n",
    "    # drop the column that remains unscaled\n",
    "    CRLA_column = df[[\"capital_run_length_average\"]]\n",
    "    # scale the rest\n",
    "    df = df.drop(columns=\"capital_run_length_average\")\n",
    "    if fit_type == \"fit_transform\":\n",
    "        scaled_columns = sc.fit_transform(df)\n",
    "    else:\n",
    "        scaled_columns = sc.transform(df)\n",
    "    # create new dataframe with scaled columns\n",
    "    df = pd.DataFrame(scaled_columns, index=df.index, columns=df.columns)\n",
    "    # concatenate and return\n",
    "    return pd.concat([CRLA_column, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = scaleAllColumnsExceptCRLA(train, sc, \"fit_transform\")\n",
    "test_scaled = scaleAllColumnsExceptCRLA(test, sc, \"fit_transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit KNR to non-NaN frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor as knr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitKNRToData(notNaNData):\n",
    "    regressor = knr(n_neighbors=15)\n",
    "    X = notNaNData.drop(columns=\"capital_run_length_average\")\n",
    "    y = notNaNData[\"capital_run_length_average\"]\n",
    "    regressor.fit(X, y)\n",
    "    return regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPredictionsFromRegressor(regressor, NaNData):\n",
    "    predictions = regressor.predict(NaNData.drop(columns=\"capital_run_length_average\"))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinePredictionsWithNaNData(predictions, NaNData):\n",
    "    NaNData = NaNData.drop(columns=\"capital_run_length_average\")\n",
    "    NaNData[\"capital_run_length_average\"] = predictions\n",
    "    return NaNData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputePipeline(df):\n",
    "    # split into NaN and non-NaN rows\n",
    "    not_nan_data = df.dropna()\n",
    "    nan_data = df[np.isnan(df.capital_run_length_average)]\n",
    "                  \n",
    "    # fit KNR to non-NaN rows\n",
    "    regressor = fitKNRToData(not_nan_data)\n",
    "    \n",
    "    # make predictions\n",
    "    predictions = createPredictionsFromRegressor(regressor, nan_data)\n",
    "    \n",
    "    # add in to the previously NaN data\n",
    "    imputed = combinePredictionsWithNaNData(predictions, nan_data)\n",
    "    \n",
    "    # sort imputed and non_nan\n",
    "    imputed = imputed.reindex(sorted(imputed.columns), axis=1)\n",
    "    not_nan_data = not_nan_data.reindex(sorted(not_nan_data.columns), axis=1)\n",
    "    \n",
    "    # rejoin\n",
    "    rejoin = pd.concat([not_nan_data, imputed])\n",
    "    return rejoin          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled_imputed = imputePipeline(train_scaled)\n",
    "test_scaled_imputed = imputePipeline(test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b\n",
    "\n",
    "Write a function named `knnclass()` that performs k-nearest neighbors classification, without resorting to a package. Essentially, we are asking you to recreate the <i>sklearn.neighbors.KNeighborsClassifier</i> function; though, we do not expect you to implement a fancy nearest neighbor search algorithm like what <i>KNeighborsClassifier</i> uses, just the naive search will suffice. Additionally, this function will be more sophisticated in the following way:\n",
    "\n",
    "* The function should automatically do a split of the training data into a sub-training set (80%) and a validation set (20%) for selecting the optimal $k$.(More sophisticated cross-validation is not necessary.)\n",
    "\n",
    "* The function should standardize each column: for a particular variable, say $x_1$, compute the mean and standard deviation of $x_1$ **using the training set only**, say $\\bar x_1$ and $s_1$; then for each observed $x_1$ in the training set and test set, subtract $\\bar x_1$, then divide by $s_1$.\n",
    "\n",
    "_Note: You can assume that all columns will be numeric and that Euclidean distance is the distance measure._\n",
    "\n",
    "The function skeleton is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnnClass:\n",
    "        \n",
    "    def __init__(self, xtrain, ytrain):\n",
    "        \n",
    "        # training data\n",
    "        self.x_train = xtrain\n",
    "        self.y_train = ytrain\n",
    "  \n",
    "        # do a further split of the training data\n",
    "        # so x_train_train, y_train_train to train model\n",
    "        # x_train_test, y_train_test to validate model\n",
    "        self.x_train_train, self.x_train_test, self.y_train_train, self.y_train_test = train_test_split(self.x_train, self.y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "        # after fit() is called, this is updated\n",
    "        self.optimal_k = -1\n",
    "        \n",
    "        \n",
    "    def euclideanDistance(self, arr1, arr2):\n",
    "        return np.linalg.norm(arr1 - arr2)\n",
    "    \n",
    "    def runTests(self):\n",
    "        # test euclideanDistance\n",
    "        print(self.euclideanDistance(self.x_train.iloc[0], self.x_train.iloc[43]))\n",
    "    \n",
    "    # dataSet: DataFrame\n",
    "    # dataPoint: Series or Numpy Array\n",
    "    def getNeighbors(self, dataSet, dataPoint, kNeighbors):\n",
    "        distances = []\n",
    "        # iterate through the data set, calculating distances to all neighbors\n",
    "        for index in range(len(dataSet)):\n",
    "            currRow = dataSet.iloc[index]\n",
    "            dist = self.euclideanDistance(currRow, dataPoint)\n",
    "            distances.append({\n",
    "                \"dist\": dist,\n",
    "                \"index\" : index\n",
    "            })\n",
    "        distances.sort(key=lambda x: x[\"dist\"]) # sort by distance\n",
    "        return distances[:kNeighbors]\n",
    "\n",
    "    def classifyUsingNeighbors(self, labelsData, neighbors):\n",
    "        # keep counters to record votes\n",
    "        spam = 0 \n",
    "        not_spam = 0\n",
    "        # iterate through the neigbors\n",
    "        for n in neighbors:\n",
    "            index = n[\"index\"]\n",
    "            # check the index against the labels\n",
    "            if labelsData.iloc[index] > 1.0:\n",
    "                spam += 1\n",
    "            else: \n",
    "                not_spam += 1\n",
    "        return 1 if spam > not_spam else 0\n",
    "    \n",
    "\n",
    "    def getAccuracy(self, trueLabels, predictedLabels):\n",
    "        accurate = 0\n",
    "        total = len(trueLabels)\n",
    "        for i in range(len(trueLabels)):\n",
    "            if trueLabel[i] and predictedLabel[i] >= 1:\n",
    "                accurate += 1\n",
    "            elif trueLabel[i] and predictedLabel[i] < 1:\n",
    "                accurate +=1 \n",
    "        return accurate / total\n",
    "    \n",
    "    def fit(self):\n",
    "        # find the best k from 1:15 using the sub training data\n",
    "        accuracies = []\n",
    "        for kNeighbors in range(1, 15):\n",
    "            # classify \n",
    "            spamLabels = []\n",
    "            for index in range(len(self.x_train_test)):\n",
    "                dataPoint = self.x_train_test.iloc[index]\n",
    "                neighbors = self.getNeighbors(self.x_train_train, dataPoint, kNeighbors)\n",
    "                label = self.classifyUsingNeighbors(self.y_train_train, neighbors)\n",
    "                spamLabels.append(label)\n",
    "            # record accuracy for this K\n",
    "            accuracies.append({\n",
    "                \"k\" : kNeighbors,\n",
    "                \"accuracy\" : getAccuracy(self.y_train_test, spamLabels)\n",
    "            })\n",
    "        # update k with best accuracy\n",
    "        accuracies.sort(key=lambda a: a[\"accuracy\"], reverse=True)\n",
    "        self.optimal_k = accuracies[\"k\"]\n",
    "            \n",
    "    \n",
    "    def predict(self, x_test, k=-1):\n",
    "        k = self.optimal_k if k < 0 else k\n",
    "        spamLabels = []\n",
    "        for index in range(len(x_test)):\n",
    "            dataPoint = x_test.iloc[index]\n",
    "            neighbors = self.getNeighbors(self.x_train, dataPoint, k)\n",
    "            label = self.classifyUsingNeighbors(self.y_train, neighbors)\n",
    "            spamLabels.append(label)\n",
    "        return spamLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_knn = train_scaled_imputed.drop(columns=[\"spam\"]) # DataFrame\n",
    "x_test_knn = test_scaled_imputed # DataFrame\n",
    "y_train_knn = train_scaled_imputed[\"spam\"] # Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "KC = KnnClass(x_train_knn, y_train_knn)\n",
    "# KC.fit()\n",
    "# KC.predict(x_test_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 74.5331 s\n",
       "File: <ipython-input-197-771ec6685ebf>\n",
       "Function: fit at line 74\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "    74                                               def fit(self):\n",
       "    75                                                   # find the best k using the split data\n",
       "    76                                                   #     accuracies = []\n",
       "    77                                                   #     for kNeighbors in range(1, 51):\n",
       "    78         1          2.0      2.0      0.0          spamLabels = []\n",
       "    79        51         42.0      0.8      0.0          for index in range(len(self.x_train_test) - 594):\n",
       "    80        50      12210.0    244.2      0.0              dataPoint = self.x_train_test.iloc[index]\n",
       "    81        50   74505510.0 1490110.2    100.0              neighbors = self.getNeighbors(self.x_train_train, dataPoint, 15)\n",
       "    82        50      15319.0    306.4      0.0              label = self.classifyUsingNeighbors(self.y_train_train, neighbors)\n",
       "    83        50         56.0      1.1      0.0              spamLabels.append(label)\n",
       "    84         1          0.0      0.0      0.0          return spamLabels"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f KC.fit KC.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Naive search for neighbors is too slow on a training set of size 3220.**\n",
    "- So for Part C, I will be using a built-in package which utilize trees and cache for fast search.\n",
    "- My Class works, but it takes 12 min for a single K, so utilizing it for all K is not feasible.\n",
    "- You can take a look at the profiler, and you will see that getting the neighbors takes close to 100% of the time. \n",
    "- It takes 1 minute on my laptop to classify 50 points. By this estimate, fitting the model for single K will take around 15 minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried till k = 5, and my optimal k was k == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c\n",
    "\n",
    "In this part, you will need to use a $k$-NN classifier to fit models on the actual dataset. If you weren't able to successfully write a $k$-NN classifier in Part b, you're permitted to use a built-in package for it. If you take this route, you may need to write some code to standardize the variables and select $k$, which `knnclass()` from part b already does. \n",
    "\n",
    "Now fit 4 models and produce 4 sets of predictions of `spam` on the test set:\n",
    "\n",
    "1. `knnclass()` using all predictors except for `capital_run_length_average` (say, if we were distrustful of our imputation approach). Call these predictions `knn_pred1`.\n",
    "\n",
    "2. `knnclass()` using all predictors including `capital_run_length_average` with the imputed values. Call these predictions `knn_pred2`.\n",
    "\n",
    "3. logistic regression using all predictors except for `capital_run_length_average`. Call these predictions `logm_pred1`.\n",
    "\n",
    "4. logistic regression using all predictors including `capital_run_length_average` with the imputed values. Call these predictions `logm_pred2`.\n",
    "\n",
    "In 3-4 sentences, provide a quick summary of your second logistic regression model (model 4). Which predictors appeared to be most significant? Are there any surprises in the predictors that ended up being significant or not significant?\n",
    "\n",
    "Submit a .csv file called `assn2_NETID_results.csv` that contains 5 columns:\n",
    "\n",
    "* `capital_run_length_average`: the predictor in your test set that now contains the imputed values (so that we can check your work on imputation).\n",
    "\n",
    "* `knn_pred1`\n",
    "\n",
    "* `knn_pred2`\n",
    "\n",
    "* `logm_pred1`\n",
    "\n",
    "* `logm_pred2`\n",
    "\n",
    "Make sure that row 1 here corresponds to row 1 of the test set, row 2 corresponds to row 2 of the test set, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN - all predictors except capital_run_length_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnClassifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knnClassifier.fit(x_train_knn.drop(columns=[\"capital_run_length_average\"]).values, [1 if i > 1 else 0 for i in y_train_knn.values])\n",
    "knn_pred1 = knnClassifier.predict(x_test_knn.drop(columns=[\"capital_run_length_average\"]).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### KNN - all predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnClassifier2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knnClassifier2.fit(x_train_knn.values, [1 if i > 1 else 0 for i in y_train_knn.values])\n",
    "knn_pred2 = knnClassifier2.predict(x_test_knn.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - all predictors except capital_run_length_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarimabbas/Developer/dataScience/sds355_container/sds355_env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lrClassifier = LogisticRegression()\n",
    "lrClassifier.fit(x_train_knn.drop(columns=[\"capital_run_length_average\"]).values, [1 if i > 1 else 0 for i in y_train_knn.values])\n",
    "logm_pred1 = lrClassifier.predict(x_test_knn.drop(columns=[\"capital_run_length_average\"]).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - all predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarimabbas/Developer/dataScience/sds355_container/sds355_env/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lrClassifier2 = LogisticRegression()\n",
    "lrModel2 = lrClassifier2.fit(x_train_knn.values, [1 if i > 1 else 0 for i in y_train_knn.values])\n",
    "logm_pred2 = lrClassifier2.predict(x_test_knn.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significant predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>word_freq_000</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_re</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_report</th>\n",
       "      <th>word_freq_table</th>\n",
       "      <th>word_freq_technology</th>\n",
       "      <th>word_freq_telnet</th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_you</th>\n",
       "      <th>word_freq_your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.758000</td>\n",
       "      <td>-0.027690</td>\n",
       "      <td>1.247501</td>\n",
       "      <td>-0.303582</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>-0.266994</td>\n",
       "      <td>0.195161</td>\n",
       "      <td>0.598068</td>\n",
       "      <td>0.437804</td>\n",
       "      <td>-0.282594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255384</td>\n",
       "      <td>-0.296400</td>\n",
       "      <td>-0.292676</td>\n",
       "      <td>-0.171399</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>0.161125</td>\n",
       "      <td>-0.173664</td>\n",
       "      <td>-0.451422</td>\n",
       "      <td>-0.811205</td>\n",
       "      <td>-0.544476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.687000</td>\n",
       "      <td>-0.199141</td>\n",
       "      <td>-0.404020</td>\n",
       "      <td>-0.034317</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>-0.337192</td>\n",
       "      <td>0.351862</td>\n",
       "      <td>-0.164188</td>\n",
       "      <td>-0.167092</td>\n",
       "      <td>-0.282594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306056</td>\n",
       "      <td>-0.296400</td>\n",
       "      <td>-0.292676</td>\n",
       "      <td>-0.171399</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>8.553064</td>\n",
       "      <td>9.710157</td>\n",
       "      <td>-0.638151</td>\n",
       "      <td>-0.927329</td>\n",
       "      <td>-0.677561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>-0.213042</td>\n",
       "      <td>-0.347180</td>\n",
       "      <td>-0.114308</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>-0.337192</td>\n",
       "      <td>0.099003</td>\n",
       "      <td>-0.164188</td>\n",
       "      <td>-0.167092</td>\n",
       "      <td>-0.282594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306056</td>\n",
       "      <td>-0.296400</td>\n",
       "      <td>-0.292676</td>\n",
       "      <td>-0.171399</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>-0.232631</td>\n",
       "      <td>-0.173664</td>\n",
       "      <td>0.598928</td>\n",
       "      <td>-0.341181</td>\n",
       "      <td>-0.677561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.038000</td>\n",
       "      <td>0.032549</td>\n",
       "      <td>-0.239815</td>\n",
       "      <td>-0.197678</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>1.876390</td>\n",
       "      <td>0.173792</td>\n",
       "      <td>-0.164188</td>\n",
       "      <td>-0.167092</td>\n",
       "      <td>3.913258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210799</td>\n",
       "      <td>-0.296400</td>\n",
       "      <td>-0.292676</td>\n",
       "      <td>-0.171399</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>-0.232631</td>\n",
       "      <td>-0.173664</td>\n",
       "      <td>0.552245</td>\n",
       "      <td>0.488274</td>\n",
       "      <td>1.451792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.785000</td>\n",
       "      <td>-0.175972</td>\n",
       "      <td>-0.328233</td>\n",
       "      <td>-0.303582</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>-0.337192</td>\n",
       "      <td>0.508563</td>\n",
       "      <td>-0.164188</td>\n",
       "      <td>1.208558</td>\n",
       "      <td>-0.282594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575638</td>\n",
       "      <td>-0.296400</td>\n",
       "      <td>-0.292676</td>\n",
       "      <td>-0.171399</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>-0.232631</td>\n",
       "      <td>-0.173664</td>\n",
       "      <td>-0.638151</td>\n",
       "      <td>-0.927329</td>\n",
       "      <td>0.046086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.098133</td>\n",
       "      <td>-0.171338</td>\n",
       "      <td>-0.219289</td>\n",
       "      <td>-0.303582</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>-0.337192</td>\n",
       "      <td>0.323371</td>\n",
       "      <td>-0.164188</td>\n",
       "      <td>-0.167092</td>\n",
       "      <td>-0.282594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306056</td>\n",
       "      <td>-0.296400</td>\n",
       "      <td>-0.292676</td>\n",
       "      <td>-0.171399</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>1.563883</td>\n",
       "      <td>-0.173664</td>\n",
       "      <td>0.213799</td>\n",
       "      <td>0.897472</td>\n",
       "      <td>-0.677561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3207</td>\n",
       "      <td>2.349533</td>\n",
       "      <td>-0.111099</td>\n",
       "      <td>0.088594</td>\n",
       "      <td>-0.285556</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>-0.037680</td>\n",
       "      <td>-0.328363</td>\n",
       "      <td>-0.164188</td>\n",
       "      <td>-0.167092</td>\n",
       "      <td>1.719349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306056</td>\n",
       "      <td>-0.296400</td>\n",
       "      <td>-0.045160</td>\n",
       "      <td>0.423743</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>-0.232631</td>\n",
       "      <td>-0.173664</td>\n",
       "      <td>0.085423</td>\n",
       "      <td>-0.927329</td>\n",
       "      <td>-0.677561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3211</td>\n",
       "      <td>2.540800</td>\n",
       "      <td>-0.083296</td>\n",
       "      <td>-0.252446</td>\n",
       "      <td>-0.232604</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>0.252472</td>\n",
       "      <td>-0.050575</td>\n",
       "      <td>0.115010</td>\n",
       "      <td>-0.167092</td>\n",
       "      <td>-0.282594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306056</td>\n",
       "      <td>-0.296400</td>\n",
       "      <td>-0.292676</td>\n",
       "      <td>-0.171399</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>-0.232631</td>\n",
       "      <td>-0.173664</td>\n",
       "      <td>0.202129</td>\n",
       "      <td>-0.529190</td>\n",
       "      <td>0.229077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3212</td>\n",
       "      <td>2.968800</td>\n",
       "      <td>0.083520</td>\n",
       "      <td>0.447003</td>\n",
       "      <td>0.064827</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>-0.337192</td>\n",
       "      <td>1.958048</td>\n",
       "      <td>-0.164188</td>\n",
       "      <td>-0.167092</td>\n",
       "      <td>-0.282594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306056</td>\n",
       "      <td>0.976208</td>\n",
       "      <td>0.350866</td>\n",
       "      <td>-0.171399</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>0.407223</td>\n",
       "      <td>-0.173664</td>\n",
       "      <td>-0.638151</td>\n",
       "      <td>-0.197408</td>\n",
       "      <td>0.195806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3217</td>\n",
       "      <td>1.555933</td>\n",
       "      <td>-0.231577</td>\n",
       "      <td>-0.437176</td>\n",
       "      <td>-0.303582</td>\n",
       "      <td>-0.126696</td>\n",
       "      <td>-0.337192</td>\n",
       "      <td>-0.499310</td>\n",
       "      <td>-0.164188</td>\n",
       "      <td>-0.167092</td>\n",
       "      <td>-0.282594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306056</td>\n",
       "      <td>-0.296400</td>\n",
       "      <td>-0.292676</td>\n",
       "      <td>-0.171399</td>\n",
       "      <td>-0.070251</td>\n",
       "      <td>-0.232631</td>\n",
       "      <td>-0.173664</td>\n",
       "      <td>-0.638151</td>\n",
       "      <td>-0.927329</td>\n",
       "      <td>-0.677561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3220 rows √ó 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       2.758000                   -0.027690   \n",
       "1                       1.687000                   -0.199141   \n",
       "2                       1.750000                   -0.213042   \n",
       "3                       5.038000                    0.032549   \n",
       "4                       1.785000                   -0.175972   \n",
       "...                          ...                         ...   \n",
       "3200                    2.098133                   -0.171338   \n",
       "3207                    2.349533                   -0.111099   \n",
       "3211                    2.540800                   -0.083296   \n",
       "3212                    2.968800                    0.083520   \n",
       "3217                    1.555933                   -0.231577   \n",
       "\n",
       "      capital_run_length_total  char_freq_!  char_freq_#  char_freq_$  \\\n",
       "0                     1.247501    -0.303582    -0.126696    -0.266994   \n",
       "1                    -0.404020    -0.034317    -0.126696    -0.337192   \n",
       "2                    -0.347180    -0.114308    -0.126696    -0.337192   \n",
       "3                    -0.239815    -0.197678    -0.126696     1.876390   \n",
       "4                    -0.328233    -0.303582    -0.126696    -0.337192   \n",
       "...                        ...          ...          ...          ...   \n",
       "3200                 -0.219289    -0.303582    -0.126696    -0.337192   \n",
       "3207                  0.088594    -0.285556    -0.126696    -0.037680   \n",
       "3211                 -0.252446    -0.232604    -0.126696     0.252472   \n",
       "3212                  0.447003     0.064827    -0.126696    -0.337192   \n",
       "3217                 -0.437176    -0.303582    -0.126696    -0.337192   \n",
       "\n",
       "      char_freq_(  char_freq_;  char_freq_[  word_freq_000  ...  word_freq_re  \\\n",
       "0        0.195161     0.598068     0.437804      -0.282594  ...     -0.255384   \n",
       "1        0.351862    -0.164188    -0.167092      -0.282594  ...     -0.306056   \n",
       "2        0.099003    -0.164188    -0.167092      -0.282594  ...     -0.306056   \n",
       "3        0.173792    -0.164188    -0.167092       3.913258  ...      0.210799   \n",
       "4        0.508563    -0.164188     1.208558      -0.282594  ...      0.575638   \n",
       "...           ...          ...          ...            ...  ...           ...   \n",
       "3200     0.323371    -0.164188    -0.167092      -0.282594  ...     -0.306056   \n",
       "3207    -0.328363    -0.164188    -0.167092       1.719349  ...     -0.306056   \n",
       "3211    -0.050575     0.115010    -0.167092      -0.282594  ...     -0.306056   \n",
       "3212     1.958048    -0.164188    -0.167092      -0.282594  ...     -0.306056   \n",
       "3217    -0.499310    -0.164188    -0.167092      -0.282594  ...     -0.306056   \n",
       "\n",
       "      word_freq_receive  word_freq_remove  word_freq_report  word_freq_table  \\\n",
       "0             -0.296400         -0.292676         -0.171399        -0.070251   \n",
       "1             -0.296400         -0.292676         -0.171399        -0.070251   \n",
       "2             -0.296400         -0.292676         -0.171399        -0.070251   \n",
       "3             -0.296400         -0.292676         -0.171399        -0.070251   \n",
       "4             -0.296400         -0.292676         -0.171399        -0.070251   \n",
       "...                 ...               ...               ...              ...   \n",
       "3200          -0.296400         -0.292676         -0.171399        -0.070251   \n",
       "3207          -0.296400         -0.045160          0.423743        -0.070251   \n",
       "3211          -0.296400         -0.292676         -0.171399        -0.070251   \n",
       "3212           0.976208          0.350866         -0.171399        -0.070251   \n",
       "3217          -0.296400         -0.292676         -0.171399        -0.070251   \n",
       "\n",
       "      word_freq_technology  word_freq_telnet  word_freq_will  word_freq_you  \\\n",
       "0                 0.161125         -0.173664       -0.451422      -0.811205   \n",
       "1                 8.553064          9.710157       -0.638151      -0.927329   \n",
       "2                -0.232631         -0.173664        0.598928      -0.341181   \n",
       "3                -0.232631         -0.173664        0.552245       0.488274   \n",
       "4                -0.232631         -0.173664       -0.638151      -0.927329   \n",
       "...                    ...               ...             ...            ...   \n",
       "3200              1.563883         -0.173664        0.213799       0.897472   \n",
       "3207             -0.232631         -0.173664        0.085423      -0.927329   \n",
       "3211             -0.232631         -0.173664        0.202129      -0.529190   \n",
       "3212              0.407223         -0.173664       -0.638151      -0.197408   \n",
       "3217             -0.232631         -0.173664       -0.638151      -0.927329   \n",
       "\n",
       "      word_freq_your  \n",
       "0          -0.544476  \n",
       "1          -0.677561  \n",
       "2          -0.677561  \n",
       "3           1.451792  \n",
       "4           0.046086  \n",
       "...              ...  \n",
       "3200       -0.677561  \n",
       "3207       -0.677561  \n",
       "3211        0.229077  \n",
       "3212        0.195806  \n",
       "3217       -0.677561  \n",
       "\n",
       "[3220 rows x 57 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = []\n",
    "for index, cf in enumerate(lrModel2.coef_[0]):\n",
    "    coeffs.append({\n",
    "        \"index\" : index,\n",
    "        \"cf\" : cf\n",
    "    })\n",
    "coeffs.sort(key=lambda x : x[\"cf\"], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 1, 'cf': 1.7143782559410765},\n",
       " {'index': 5, 'cf': 0.9539694052764146},\n",
       " {'index': 9, 'cf': 0.8935906448660724},\n",
       " {'index': 49, 'cf': 0.7802056135918461},\n",
       " {'index': 28, 'cf': 0.7452619483387634},\n",
       " {'index': 11, 'cf': 0.7424063561987302},\n",
       " {'index': 4, 'cf': 0.6443895856549169},\n",
       " {'index': 21, 'cf': 0.5793399173060575},\n",
       " {'index': 38, 'cf': 0.43386436294173547},\n",
       " {'index': 19, 'cf': 0.42524857740901895},\n",
       " {'index': 41, 'cf': 0.3551553040203605},\n",
       " {'index': 56, 'cf': 0.33641363429235827},\n",
       " {'index': 52, 'cf': 0.3333377244536883},\n",
       " {'index': 2, 'cf': 0.2986137669190685},\n",
       " {'index': 27, 'cf': 0.2771960391204137},\n",
       " {'index': 17, 'cf': 0.27671800946342234},\n",
       " {'index': 32, 'cf': 0.24964294203130896},\n",
       " {'index': 3, 'cf': 0.23769801745698302},\n",
       " {'index': 42, 'cf': 0.18515181862471466},\n",
       " {'index': 13, 'cf': 0.15383402259570236},\n",
       " {'index': 55, 'cf': 0.13989332669603638},\n",
       " {'index': 35, 'cf': 0.12274363237634585},\n",
       " {'index': 26, 'cf': 0.1066451288376731},\n",
       " {'index': 18, 'cf': 0.10445741975210442},\n",
       " {'index': 39, 'cf': 0.07452521026414054},\n",
       " {'index': 0, 'cf': 0.07375305989089152},\n",
       " {'index': 50, 'cf': 0.04517879625872991},\n",
       " {'index': 6, 'cf': 0.01837790684119818},\n",
       " {'index': 10, 'cf': 0.000325806581166473},\n",
       " {'index': 48, 'cf': -0.04718301433236289},\n",
       " {'index': 44, 'cf': -0.05323241519665686},\n",
       " {'index': 51, 'cf': -0.0603959177836788},\n",
       " {'index': 36, 'cf': -0.0749952542140989},\n",
       " {'index': 54, 'cf': -0.08898849450571826},\n",
       " {'index': 12, 'cf': -0.10800028917584115},\n",
       " {'index': 43, 'cf': -0.11329307077837648},\n",
       " {'index': 34, 'cf': -0.11834382781751411},\n",
       " {'index': 40, 'cf': -0.15032089437332502},\n",
       " {'index': 8, 'cf': -0.15104702422603858},\n",
       " {'index': 15, 'cf': -0.21805066262971576},\n",
       " {'index': 16, 'cf': -0.2321104941102357},\n",
       " {'index': 24, 'cf': -0.26341591150844496},\n",
       " {'index': 7, 'cf': -0.2949123739999225},\n",
       " {'index': 23, 'cf': -0.4215542945405113},\n",
       " {'index': 45, 'cf': -0.4634880016191258},\n",
       " {'index': 53, 'cf': -0.5863738148024542},\n",
       " {'index': 14, 'cf': -0.6203610088575519},\n",
       " {'index': 46, 'cf': -0.7956938888488201},\n",
       " {'index': 47, 'cf': -0.828142034847825},\n",
       " {'index': 20, 'cf': -0.8946151151870269},\n",
       " {'index': 31, 'cf': -0.8996963233425389},\n",
       " {'index': 33, 'cf': -0.9572149549841147},\n",
       " {'index': 22, 'cf': -1.1292032323796697},\n",
       " {'index': 25, 'cf': -1.1759474676644763},\n",
       " {'index': 37, 'cf': -1.3212986174453196},\n",
       " {'index': 30, 'cf': -2.2036224797398627},\n",
       " {'index': 29, 'cf': -3.48890040621353}]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these standardized coefficients, significant predictors include:\n",
    "\n",
    "- capital_run_length_total\n",
    "- char_freq_$\n",
    "- word_freq_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportDict = {\n",
    "    \"capital_run_length_average\" :  x_test_knn[\"capital_run_length_average\"].values,\n",
    "    \"knn_pred1\" : knn_pred1,\n",
    "    \"knn_pred2\" : knn_pred2,\n",
    "    \"logm_pred1\" : logm_pred1,\n",
    "    \"logm_pred2\" : logm_pred2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exportDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ab4f0adfe26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexportFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexportDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SalePrice\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'exportDict' is not defined"
     ]
    }
   ],
   "source": [
    "exportFrame = pd.DataFrame(exportDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportFrame.to_csv(\"./file.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the scenario of univariate logistic regression where we are trying to predict $Y$, which can take the value $0$ or $1$, from the variable $X$, which can take the value of any real number. Recall from lecture that we need to predict parameters $\\beta_{0}$ and $\\beta_{1}$ by minimizing the penalized loss function:\n",
    "\n",
    "$L(\\beta_{0}, \\beta_{1}) = \\sum\\limits_{i=1}^{n} \\left[ log\\left( 1 + e^{\\beta_{0} + X_{i}\\beta_{1}}\\right) - Y_{i}\\left(\\beta_{0} + X_{i}\\beta_{1}\\right)\\right] + \\lambda\\left(\\beta_{0}^{2} + \\beta_{1}^{2}\\right)$ .\n",
    "\n",
    "Run the next cell to simulate data from the true values of $\\beta_{0} = 2.5$ and $\\beta_{1} = 3.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000 \n",
    "x1 = np.random.uniform(-5, 5, size=n) \n",
    "beta0 = 2.5\n",
    "beta1 = -3.0 \n",
    "p = np.exp(beta0 + x1*beta1)/(1 + np.exp(beta0 + beta1*x1))\n",
    "y = np.random.binomial(1, p, size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.6447943 , -1.77319003,  1.70788791, ...,  4.44073042,\n",
       "       -3.98045186,  0.82309827])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2.17219945e-04, 9.99598367e-01, 6.76309040e-02, ...,\n",
       "       1.99514009e-05, 9.99999465e-01, 5.07675693e-01])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x1)\n",
    "display(p)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a\n",
    "\n",
    "For given values of $\\beta_{0}$ and $\\beta_{1}$ the vector $\\left( \\dfrac{\\partial}{\\partial \\beta_{0}} L(\\beta_{0}, \\beta_{1}), \\dfrac{\\partial}{\\partial \\beta_{1}} L(\\beta_{0}, \\beta_{1}) \\right)^{T}$ is called the gradient of $L(\\beta_{0}, \\beta_{1})$ and is denoted $\\nabla L(\\beta_{0}, \\beta_{1})$.\n",
    "\n",
    "Calculate the derivative of $L(\\beta_{0}, \\beta_{1})$ with respect to $\\beta_{0}$, treating $\\beta_{1}$ as a constant. (i.e. calculate $\\dfrac{\\partial}{\\partial \\beta_{0}} L(\\beta_{0}, \\beta_{1})$). \n",
    "\n",
    "Now calculate the derivative of $L(\\beta_{0}, \\beta_{1})$ with respect to $\\beta_{1}$, treating $\\beta_{0}$ as a constant. (i.e. calculate $\\dfrac{\\partial}{\\partial \\beta_{1}} L(\\beta_{0}, \\beta_{1})$).\n",
    "\n",
    "Be sure to show your work by either typing it in here using LaTeX, or by taking a picture of your handwritten solutions and displaying them here in the notebook. (If you choose the latter of these two options, be sure that the display is large enough and legible. You may find the example shown in the <i>Introduction to Python.ipynb</i> notebook for the Yale image useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## please put your answer here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function in the following cell called <i>update()</i> which takes values for $\\beta_{0}$ and $\\beta_{1}$ as well as a step-size $\\eta$ and should return updated values for $\\beta_{0}$ and $\\beta_{1}$ from one step of gradient descent (using all the data and your answer to Part a). You may use the value $0.01$ for $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update(b0, b1, eta):\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the function in the next cell called <i>loss()</i> which takes values for $\\beta_{0}$ and $\\beta_{1}$ and should return the value of the loss function evaluated at those two parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def loss(b0, b1):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell uses the two functions from Part b to implement gradient descent for this problem, keeping track of the values for $\\beta_{0}$, $\\beta_{1}$, and $L(\\beta_{0}, \\beta_{1})$ at each iteration. In the cell below the code, answer each of the questions included as comments next to the code. Also, create individual plots of $\\beta_{0}$, $\\beta_{1}$, and $L(\\beta_{0}, \\beta_{1})$ vs. iteration number. Do these three quantities behave as expected for gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "step = 0.01\n",
    "beta0_hat = 0\n",
    "beta1_hat = 0\n",
    "l = loss(beta0_hat, beta1_hat)\n",
    "beta0_all = [beta0_hat]\n",
    "beta1_all = [beta1_hat]\n",
    "loss_all = [l]\n",
    "i=0\n",
    "while i < 400 and step > 3e-8:              #1. What is the reasoning behind these two stopping criteria?\n",
    "    b = update(beta0_hat, beta1_hat, step)  #2. What is being calculated here?\n",
    "    l_new = loss(b[0], b[1])                #3. What is being calculated here?\n",
    "    if l_new < l:                           #4. What happens if the statement being tested here is True?\n",
    "        beta0_hat = b[0]\n",
    "        beta1_hat = b[1]\n",
    "        l = l_new\n",
    "    else:\n",
    "        step = step*0.9                     #5. What happens if the statement tested above is False? What is the reasoning\n",
    "                                            #   behind this?\n",
    "    i = i+1\n",
    "    beta0_all.append(beta0_hat)\n",
    "    beta1_all.append(beta1_hat)\n",
    "    loss_all.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4.\n",
    "\n",
    "5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Create your plots here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your gradient descent algorithm from Part b robust against initial estimates of $\\beta_{0}$ and $\\beta_{1}$? To help answer this question, take the code above that implements gradient descent and put it in a function that takes initial estimates of $\\beta_{0}$ and $\\beta_{1}$ as arguments, and returns the optimized values from gradient descent. Run this function using each of the following pairs of $(\\beta_{0}, \\beta_{1})$ as initial estimates: $(15, 3)$, $(-30, 5)$, and $(-8, -8)$. Are your final estimates approximately the same each time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## please put your answer here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Cross-Validation\n",
    "\n",
    "## Part a\n",
    "\n",
    "Generate a simulated data set with the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.normal(size=100) # 100 random normal points\n",
    "y = x - 2*x**2 + np.random.normal(size=100) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set, what is the value of $n$ (the number of data points) and what is the value of $p$ (the true number of model parameters)? Write out the model used to generate the data in equation form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "N = 100\n",
    "\n",
    "p = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = x - 2x^2 + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b\n",
    "\n",
    "Create a scatterplot of $X$ against $Y$. Comment on what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df5xcdX3v8dcny0KXn6tC1WwIwaJBIJXIgtSorWgNUsWIl6p9tD64rY1a6wUKaYN4a2jrJZpa2tva2ig8qI+iBARWEHuDPIK1cOVHQhJjgFgEI2zwGgoLgSywST73j3MmmZ2cM3Nm5sx8z8x5Px+PfbCZOTPnM7vL93O+P87na+6OiIiUz4zQAYiISBhKACIiJaUEICJSUkoAIiIlpQQgIlJSSgAiIiWlBCASMzM3s+NCx5E3M3vOzF4TOg4pHiUAaYqZvcXM/q+ZPWNmT5nZXWZ2apvveZ6Z3Vnz2NVm9lftRdsZSfHWPD9gZvea2aU1j91nZhd3J8p93P1Qd3+knfeo9/uwyPfN7LM1j3/EzH5iZge3c27pHCUAyczMDge+Dfw98HJgBLgMeDFkXEnM7IBQ53b33cDvA39mZsfHD18MOHBFqLg6xaO7ST8KXGhmJwKY2VHAF4GPuvvOkPFJHe6uL31l+gJGgYkGx/wh8CCwA3gAeGP8+FLgJ1WPvz9+/PXAC8Bu4DlgAlgMTAEvxY/dEh87E7gB2A48CvyPqvMuA74J/CvwLFHDUxvb1cCXge/Gcfw7cEzV8w4cF39/BPC1+Fxbgc8QXTDtF2+dn8Uy4M74Nc8A8+oc+1vA+jj2x4BlNc9/JI7jv4D/CfwUeGf83GnAD+Kf3RPAPwAHpnyuq4EvAbfGP4N7gF+JnzOiBPWLOI5NwElpv4+Ez7AEuDv+OX0D+HLov1l91f8KHoC+eucLODxugP4FeDfwsprnzwXGgVPjxuS4SgMbPzczbhw+CDwPvDp+7jzgzpr3uhr4q6p/zwDWAX8OHAi8BngEWBg/vyxupBbFxw4lxH913Oi9DTgI+Lvq89Y0lF8DvgUcBswBfgz8QVq8KT+vA+NG9EngsgbH/gYwL479V4H/ByyKnzshbnjfEr/nX8eftZIATgFOBw6IY30QuCDlc10d/w5Pi4+/Brg2fm5h/DMejn9/r6/6HU37faR8hgGihHIj8DPgsNB/s/qq/6UhIMnM3Z8laoQc+Aqw3cxuNrNXxod8FPiCu9/nkYfdfWv82uvdfZu773H3VcB/EjVCWZ0KHOXuf+HuL3k0pv0V4ENVx/zA3cfic0ymvM+t7v59d38RuBT4NTM7uvoAMxuI3/cSd9/h7j8lGs74vSbixd1fImoQX0HU0NY79nvuvimO/YdEV9C/Hj/934iuuu+M3/PPiX4Hldeuc/e73X1XHOs/V702yU3ufq+774rjOjl+fIoo4R0PmLs/6O5PNPF5K0Nf7wc+5e47sr5WwlACkKbEjcJ57j6LaHhgJvC38dNHEw3z7CeeENxgZhNmNhG/9sgmTn0MMLPy+vg9Pg28suqYxzK8z95j3P054Kn4M1Q7EhgkGnKp2Eo055GZmb2VqEfyL0S9jXrHvsnM7jCz7Wb2DPBx9v18ZtbEvZPoKr7y2teZ2bfN7Odm9izwv6j/s/151fc7gUPj911DNHz0JeAXZrYynvfJzN03x99urnugFIISgLTM3R8iGho4KX7oMeBXao8zs2OIrtb/GHiFuw8DPyIaZoCqq9nqt6/592PAo+4+XPV1mLufVec1SfZe7ZvZoUST2dtqjnmS6Gr4mKrHZhMNb2U6j5kNAVcSTf5+EphrZr9b5yVfB24Gjnb3I4jmKio/nyeAWTXv/Yqq1/4T8BDwWnc/nCgxGi1w9//t7qcQDTu9jmhcH7L9bKXHKAFIZmZ2vJldZGaz4n8fDXyYaOIP4KvAxWZ2Srw08Li48T+EqAHZHr/uv7MvaUA03j3LzA6seax67fq9wA4z+zMzG4qXVZ7UwhLUs+KlrAcCfwnc7e7Teg7xUMZ1wOfM7LD4M/wJ0QRzWry1LgN+6u5Xu/vzwMeAK8ws7cr8MOApd3/BzE4DfqfquW8C7zWzN8fnXMb0Bv4woknb5+JVR59o9ENIYmanxj2RQaI5mheAPfHTtb8P6QNKANKMHcCbgHvM7Hmihv9HwEUQjfMDnyO6mt0BjAEvd/cHiMbQf0DUkMwD7qp63zVEQwY/N7Mn48euBE6Ih3vG4kb5PUTj1Y8SXaV/lWi1TjO+DnyWaOjnFCDtqvxTRI3gI0Qreb4OXFUn3r3MbJSowf9Y5TF3/y7REtq0oaA/Av7CzHYQjfFfV/XazXE81xL1Bp4jWqlTWX57MVHC2EHU01qV9uEbODx+/dPsW3G0In5u2u+jxfeXgjF39eykHMzsauBxd/9M6FjaEQ9dTRAN+TwaOh7pXeoBiPQAM3uvmR1sZocQLQPdRHQvgEjLlABEesP7iCartwGvBT7k6r5LmzQEJCJSUuoBiIiUVLCCWa048sgjfc6cOaHDEBHpKevWrXvS3Y+qfbynEsCcOXNYu3Zt6DBERHqKmW1NelxDQCIiJaUEICJSUkoAIiIlpQQgIlJSSgAiIiWlBCAiUlI9tQxUpCjG1o+zYvUWtk1MMnN4iCUL57JoflP7xYgEpwQg0qSx9eNccuMmJqd2AzA+McklN24CUBKQnqIhIJEmrVi9ZW/jXzE5tZsVq7cEikikNUoAIk3aNpG833za4yJFpSEgKY28xu1nDg8xntDYzxweyiNMka5RD0BKoTJuPz4xibNv3H5s/XjD19ZasnAuQ4MD0x4bGhxgycK5OUUr0h1KAFIKeY7bL5o/wuXnzGNkeAgDRoaHuPyceZoAlp6jISAphbzH7RfNH1GDLz0vWA/AzI42szvM7AEz22xm54eKRfpf2vi8xu2lzEIOAe0CLnL3E4DTgU+a2QkB45E+pnF7kf0FGwJy9yeAJ+Lvd5jZg8AI8EComKR/VYZrdPeuyD6F2BTezOYA3wdOcvdna55bDCwGmD179ilbtyZubCMiIinMbJ27j9Y+HnwVkJkdCtwAXFDb+AO4+0p3H3X30aOO2m9LSxERaVHQBGBmg0SN/zXufmPIWEREyibkKiADrgQedPe/CRWHiEhZhewBLAB+DzjDzDbEX2cFjEdEpFRCrgK6E7BQ5xcRKbvgk8AiIhKGEoCISEmpFpBIj9F2lJIXJQCRHqLtKCVPSgAiPaRRWWv1DKQZSgAiPSStfHWlJ6CegTRDCUCkh6RtRzlgltgzuOyWzeoVSCqtAhIpiLH14yxYvoZjl97KguVrErerTCtrvTulqOPTO6dy2QZT+pMSgEgBZN2zOG07ypGMG9u0ug2m9CcNAYkUQL3J3dohm7TtKKvnAOppdRtM6T9KACIF0O6exUkb3jz/4i4mJqf2O1bbYEqFEoBIAaRN7jbTWNf2DGrvGQBtgynTaQ5ApAA6sWdx7XzB8NAgvzQ4gwtXbUidZJZyUQ9ApAA6tWdxpVegO4gliRKASEGkTe7moZlJZikPJQCRwLpR3K3dSWbpT5oDEAko6/r/dqVNJmtFULkpAYgE1Ki4W146McksvU9DQCIBdWtoplOTzNLblABEAspj/X9WnZxklt6kISCRgDQ0IyGpByASkIZmJCQlAJHANDQjoWgISESkpNQDECmJbtxwJr1FCUCkBFQLSJIoAYiUQK/XAlLvpTOUAERKoJdrAan30jmaBBYpgVZrAWXZqL7TulUuo4zUAxApgSUL5za9O1i7V96Nhm2yDuuk9VKS7qCW5qgHIFICtbuDjQwPcfk58+o25O1ceTeqctpMFdS0XorF7yOtUw9ApCSaveGsnXmDRpPOzUxKL1k4lwtXbcBrzuHxeTQP0Dr1AEQkUTt7CDRKHs0kl0XzR/Zr/BudR7JRAhCRRO0UqmuUPJpNLiPa0KYjlABEJFEr8wYVjZJHs8lFVVM7I+gcgJldBbwH+IW7nxQyFhHZX6uF6hpVOW22CqqqpnaGuaeNrnXh5GZvA54DvpYlAYyOjvratWs7H5iISB8xs3XuPlr7eNAhIHf/PvBUyBhERMqq8MtAzWwxsBhg9uzZgaMRkU5QrZ8wCp8A3H0lsBKiIaDA4Yj0ndCNb6haP6E/dxEUPgGISOd0utxDFiEqlarAXETLQEVKLO9yDxes2sDJl93WVImGtJo+naz1owJzkaAJwMy+AfwAmGtmj5vZH4SMR6Rs8i73ADAxOZVa1yfJgFnqc3M6VIW0l8tj5yn0KqAPu/ur3X3Q3We5+5Uh4xEpm06Ue4DmrqZ3N1iKXq9QXKva+dz9RENAIiXWiXIPFVmvptPKPFTLe3hGdxZHlABESizvcg/Vsl5NN3qfijyHZ9r53P1Eq4BESq7dcg+X3bKZp3dOTXuumavpyvtcdN3GusNBRwwN7v0+j9VHrX7ufqIEICItqzSi7TbIlWNrdy2rVpkr1hLO/CgBiEjb8riarrz+glUbEp+fiHsZnbpvoIw3hmkOQEQKY9H8kYa1/zuxhLOZLSr7iRKAiBRKoxU6nVjCWdYbw5QARKRQGq3Q6cQSzrLeGKY5ABEpnHpzCnltDlM95j/DLHEFUr/fGBZ0Q5hmaUMYEamWdeK29ri3H38UN6wbT11xBFGvol/uDUjbEEY9ABHpSVmXgyYdd83dPyPp0nfAjD3uWgUkIlJkWSduk45LG/fY484VHzwZgAtXbehIIboiUQ9ARHpS1onbZspKDx88WKqbzNQDEJGelGU56Nj6cdKLTU83NDiAOx1bDjq2fpwFy9dwbIdKXLdCCUBEelKW5aArVm9JHe6pZsAHThnhmcmpxOfbXQ5a1BvNlABEpCdlqeiZteF24I6Htndsn4Ci3mimOQAR6VmNahDNHB7KPAdQOc6YPkmcxz4BRb3RTD0AEelbScNEjeYEvOqYevsENDOmX9QdyJQARKRvVQ8TQbTOv7qBT+NEjf9dS89IbfybGdMv6g5kSgAi0tcWzR/Z2wBXyj1kSQL1hmeaHdMv6g5kmgMQkb6XdjNYpWeQNE9Qb3imlTH9Iu5ApgQgIj1rbP34tC0ph4cGWXb2ifvtUpa2FHTbxCRXfPDk/XYiazQ8kza5HHpMv1lKACLSk8bWj7PkmxuZ2r2veZ+YnGLJ9RtZu/UpVt332LTnkswcHmqpuuiShXObThpFpAQgIj1pxeotiQ381B7nmnt+RqNCx9UNdrPDM3mVpA5NCUBEgmlnH9564+31Gn+D1HM1E08Rx/SbpQQgIkFkLeecppmbvKo9uvy3OhJPL1ICEJEg6i2lzNLgLlk4d785gEYOHoxWvleu9McnJhmIdwMbSNgVrJl4epESgIgE0W55hEqjXL0KqJGp3c5nxjZN2w2s0ugnbQnZTDy9SAlARILIYyll9Tj8guVrGg4JTe1xvnHPY6mNfVqcobQzR5KF7gQWkSCaKY+Qpe5O1iv1Zhr/kEs7u1FCWglARILIWh4ha0OY15X6gFkhyjV0o4S0hoBEJFd5L6XMOlmcdHNWs4YGBwpRowe6U0JaPQARyU0nhi2yNoRJPYrfPX323no/jQyY8YFTirO2vxslpNUDEJHctLu0M0kzk8VpPYosE8S73blh3Tijx7w8tQR0N+/87Ua5iaA9ADM708y2mNnDZrY0ZCwi0r5ODFvkUUt/ycK5DM5ovD182hh7iD19u1FCOlgPwMwGgC8Bvwk8DtxnZje7+wOhYhKR9uRdJbNy1T05tXvvjVojLVx9V4698LoNDWsEJSWrTvRssuh0uYmQPYDTgIfd/RF3fwm4FnhfwHhEpE157nxVfdUN0RBN5b1aaRQXzR8htS50laRkVdQ9fdsVMgGMAI9V/fvx+DER6VF5Dlt0Yhlko55IWrIq6p6+7UodAjKz2e7+s5Tn3uru/9G5sKadazGwGGD27NndOKWItCGvYYtGV92tTMomTawa+3YHS3uPfqn/X6veHMD3zOzLwBfdfTeAmb0S+CJwPDDa5rnHgaOr/j0rfmwad18JrAQYHR3NfgufiPS0evMJrVbubLWOf7/U/69lnjIjYmYvA5YDbwbOB+YBfwJ8Afgnd9/T1onNDgB+DLyDqOG/D/gdd9+c9prR0VFfu3ZtO6cVkR5R28jDvhu1KpU8a40MD3HX0jO6GWbXl4e2wszWuft+F+2pPQB3fxr4mJmdD9wObANOd/fH8wjI3XeZ2R8Dq4EB4Kp6jb+IlEu9q+4LV21IfE23J2V7fQ+BenMAw8DngTcBZwJnAf9mZue7+5o8Tu7u3wG+k8d7iUj/SZtPKMqm7KGWh+al3hzA/cA/Ap90913AbWZ2MvCPZrbV3T/clQhFRJg+1DI0mLyA8e3HH9XVIZleXx5aLwG8rXa4x903AG82sz/sbFgi0qs60QDXDrXsnEqegvz2xiembfYyPjHJBas2sOzmzSw7+8TcE0FReiKtSr0PoN5Yv7t/pTPhiEgv61TJhKShliQTk1OJx01MTnWkdEOeN76FoGqgIpKby27Z3JEa9nkMqeRdSx+6U6+nk1QNVERyMbZ+PHVv3nYb8LShlmpDgwP80uCMuvsDd2JsvtP1ejpJCUBEclHv6rqZMfGkOYQsm71cfs48gLrHZYmjF9b150VDQCKSi3pX11nHxNPmECBq4AcsuaTzyPDQ3ivxy8+Zx8sOHtzvGIvfL21P4Xrn72TZ55CUAET6WJbN1POSdnU9PDSY+Qq60br6L/72GzJNuh58YDS4UUkYlXo/UL9Rb7cAXTd/3nlQAhDpU92+mk1aEVMdSxaN1tU3mnRNKiFd3fhXpDXqWdb1pzXyvdh70ByASJ/q9l2qlfe87JbN0yZiJyanWHL9Ri67ZTMTO6fqjqtnWVdfb9I16TOnVZBMauwbnb9e6YdevCtYPQCRPhXiLtVF80f2Dr9Um9rjPL1zquGVcbvr6pv5bElDVo3OX6+R78W7gpUARPpUqE1MsjR4aUMw7a6rT/tstVPHaUml0fnrNfK9uGmMhoBE+lSoTUyyrNmH9Ma0nXX1aZ/5A6eMcMdD2zMt7ax3/npDRL24aYwSgEifCrWJSZY1+9CZK+NOf+Z6jXwvbhqTuiFMEWlDGJHeUH0z1RFDgzz/0i6mdu9rayobu+TVOHbz5q1evFEsbUMYJQAR6bhWGs2sr6m3c1jRG+ZuaXpHMBGRvDQ7rt/MTlu9uPyyKLQKSEQKp5k7cntx+WVRKAGISOE006gPJ9T9gWIvvywKJQAR6bhma+SkNd4zzKa9dmz9OM+9sGu/4wYHrNDLL4tCk8Ai0lGtTNImvaZicMA45MADeGZyihlm7E5ow4aHBtnw2Xfl9yF6nCaBRSSIRuP5tSt90l5TMbXbmZiMag0lNf4Az0ymbwoj+ygBiJRI3mvYs7xf2nh+ZWVP9UqfJd/cCB7VDmqHxv+zUQIQKYlmllbm+X5p5RMGzPa7yq++WaxVRS+/UCSaBBYpiXY3O2n1/dIqbKYN37SqFzdlD009AJGSSCvQVrvZSdYhoqxLNdNq5NTuG1DPgBl73MEgKW9o0rc1SgAiJTC2fjxxZyzIttlJq5u3VCTdCbzs5s2ZYq9eMTS2fpwl12+cNkcwOMNYdvaJmd5LptMQkEgJrFi9JbHxN8i02UmSdjdvqbdSJ60e/6L5I6w49w3Tnl9x7hs05NMi9QBESiBtuMbZd3XfbEmFdssfp/UgRoaHuGvpGamva2e/AJlOPQCREkhbFjlS9XgrO1otmj/CXUvP4IoPngzAhas2ZLrTF9rvQUj71AMQKZBO1Zqvt5FJ5ZzjE5P7zRNkaZBbXV7aixuo9BslAJGCyHudfrW0xhaYdk6HvUlgJGOD3E45Zg3nhKUEIFIQna5rn9TYLli+Zr9zVhr/euPw1VSOuXcpAYgURIiGtF6ZhgXL12QammlmOagUiyaBRQqilUnYTp3TiJKAs28oKm1iV5O5vStIAjCzc81ss5ntMbP9SpSKlFGIhjTpnEk3jNW7H2DR/BEuP2de6tp9Ka5QQ0A/As4B/jnQ+UUKJ8SqmKRzZikZkfQ+avB7T5AE4O4PAphZiNOLFFaIhrT2nAuWr9GYfkkUfg7AzBab2VozW7t9+/bQ4Yj0PY3pl0fHegBmdjvwqoSnLnX3b2V9H3dfCayEaEvInMITKa1GN5vpBq3y6FgCcPd3duq9RYqsU3fzNnuOpGOATDebaUy/HHQfgEiOOnk3bzPnSDvmoANmdPRmM+ktoZaBvt/MHgd+DbjVzFaHiEMkb3nvutXqOdKOmUgpwZy28kf6W5AE4O43ufssdz/I3V/p7gtDxCGSt27czZvlHM2ezyBTBU/pL4VfBSTSS7pxN2+Wc6Qd87KDB0lafO2Qay9FeoMSgEiOurGEMss50o757HtPTNwZDFS8rYyUAERy1I2yCFnOUe+YkQA1h6SYzL13ltaPjo762rVrQ4ch0tNqVwjB9I3Xpf+Y2Tp336/umpaBipSMbvSSCiUAkRLSjV4CmgMQESktJQARkZJSAhARKSnNAUghdaOgmkjZKQFI4XSjoJqIKAFIAdUrdtZsAlBPQiSdEoAUTl4F1dSTEKlPk8BSOHkVVOtGaWaRXqYEIIWTV0G1bpRmFullSgBSOHkVVOtGaWaRXqY5ACmkPEoVLFk4N7HoWZ6lmUV6mRKA9C0VPROpTwlA+pqKnomk0xyAiEhJKQGIiJSUEoCISEkpAYiIlJQSgIhISSkBiIiUlBKAiEhJKQGIiJSUEoCISEkpAYiIlJQSgIhISSkBiIiUlBKAiEhJKQGIiJSUEoCISEkpAYiIlFSQBGBmK8zsITP7oZndZGbDIeIQESmzUD2A7wInufuvAj8GLgkUR2GNrR9nwfI1HLv0VhYsX8PY+vHQIYlInwmSANz9NnffFf/zbmBWiDiKamz9OJfcuInxiUkcGJ+Y5JIbNykJiEiuzN3DBmB2C7DK3f815fnFwGKA2bNnn7J169ZuhhfEguVrGJ+Y3O/xATP2uGtzcxFpipmtc/fR2sc7tim8md0OvCrhqUvd/VvxMZcCu4Br0t7H3VcCKwFGR0fDZqsu2ZbQ+APsjpN1pUcAKAmISMs6lgDc/Z31njez84D3AO/w0N2Qgpk5PJTYA6g2ObWbFau3KAGISMtCrQI6E/hT4Gx33xkihiJbsnAuQ4MDDY9L6ymIiGTRsR5AA/8AHAR818wA7nb3jweKpXAqV/UrVm9h28QkM8z2Dv9Umzk81O3QRKSPBEkA7n5ciPP2kkXzR/YmgsqqoMmp3XufHxocYMnCuaHCE5E+EKoHIE2o7RFoFZCI5EEJoEdU9whERPKgWkAiIiWlBCAiUlJ9PwQ0tn5cY+ciIgn6OgHUrp7RHbQiIvv09RDQitVbpi2dhH130IqIlF1fJ4C0O2V1B62ISJ8ngLQ7ZXUHrYhInyeApJo6uoNWRCTS15PAuoNWRCRdXycA0B20IiJp+noISERE0ikBiIiUlBKAiEhJKQGIiJSUEoCISEkpAYiIlJR5wl6zRWVm24GtAU59JPBkgPNmUeTYoNjxFTk2UHztKHJs0P34jnH3o2of7KkEEIqZrXX30dBxJClybFDs+IocGyi+dhQ5NihOfBoCEhEpKSUAEZGSUgLIZmXoAOoocmxQ7PiKHBsovnYUOTYoSHyaAxARKSn1AERESkoJQESkpJQAUpjZuWa22cz2mNlo1eO/aWbrzGxT/N8zihRf/NwlZvawmW0xs4Uh4quJ52Qzu9vMNpjZWjM7LXRM1czsU2b2UPzz/ELoeJKY2UVm5mZ2ZOhYKsxsRfxz+6GZ3WRmw6FjAjCzM+O//YfNbGnoeCrM7Ggzu8PMHoj/1s4PHRPurq+EL+D1wFzge8Bo1ePzgZnx9ycB4wWL7wRgI3AQcCzwE2Ag8M/yNuDd8fdnAd8L/futiu3twO3AQfG/fzl0TAkxHg2sJroJ8sjQ8VTF9S7ggPj7zwOfL0BMA/Hf/GuAA+P/F04IHVcc26uBN8bfHwb8OHRs6gGkcPcH3X1LwuPr3X1b/M/NwJCZHdTd6NLjA94HXOvuL7r7o8DDQOgrbgcOj78/AthW59hu+wSw3N1fBHD3XwSOJ8kVwJ8S/RwLw91vc/dd8T/vBmaFjCd2GvCwuz/i7i8B1xL9PxGcuz/h7vfH3+8AHgSC7lalBNCeDwD3VxqPghgBHqv69+ME/iMDLgBWmNljwF8DlwSOp9rrgLea2T1m9u9mdmrogKqZ2fuIepkbQ8fSwO8D/xY6CIr5978fM5tDNJpwT8g4+n5LyHrM7HbgVQlPXeru32rw2hOJur3v6kRs8Tlajq/b6sUKvAO40N1vMLPfBq4E3lmQ2A4AXg6cDpwKXGdmr/G4n16A+D5NB//GGsnyN2hmlwK7gGu6GVuvMrNDgRuAC9z92ZCxlDoBuHtLjZCZzQJuAj7i7j/JN6p9WoxvnGjMuGJW/FhH1YvVzL4GVCa8rge+2ul4qjWI7RPAjXGDf6+Z7SEq1LU9dHxmNo9oHmejmUH0u7zfzE5z95+HjK3CzM4D3gO8o5tJs44gf/9ZmdkgUeN/jbvfGDoeDQE1KV7pcCuw1N3vCh1PgpuBD5nZQWZ2LPBa4N7AMW0Dfj3+/gzgPwPGUmuMaCIYM3sd0cRhIapIuvsmd/9ld5/j7nOIhjPe2K3GvxEzO5NobuJsd98ZOp7YfcBrzexYMzsQ+BDR/xPBWZTFrwQedPe/CR0P6E7gVGb2fuDvgaOACWCDuy80s88QjWFXN2Lv6vbkYVp88XOXEo3J7iLqZgYdmzWztwB/R9TjfAH4I3dfFzKmiriRuAo4GXgJuNjd14SNKpmZ/ZRoxVchEpSZPUy02uy/4ofudvePBwwJADM7C/hbohVBV7n75wKHBOz9/+A/gE3AnvjhT7v7d4LFpAQgIlJOGgISESkpJQARkZJSAhARKSklABGRklICEBEpKSUAkRZZ5E4ze3fVY+ea2f8JGZdIVloGKtIGMzuJ6O7m+UT3OawHzuzkHeIieVECEGlTvIfA88AhwA53/8vAIYlkogQg0iYzOwS4n+hO4tGCVYcVSVXqYnAieXD3581sFfCcGn/pJZoEFsnHHoUSgyAAAABESURBVPbVdxHpCUoAIiIlpQQgIlJSmgQWESkp9QBEREpKCUBEpKSUAERESkoJQESkpJQARERKSglARKSklABERErq/wMTCq967mhorwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y, x) # questions wants X against Y i.e. X on the y-axis\n",
    "plt.title('Scatter plot X against Y') \n",
    "plt.xlabel('Y')\n",
    "plt.ylabel('X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph looks quadratic, with Y maximum near X = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c\n",
    "\n",
    "Set a random seed, and then compute the Leave-One-Out Cross-Validation (LOOCV) errors that result from fitting the following four models using least squares:\n",
    "         \n",
    "i. $Y = \\beta_0 + \\beta_1 X + \\epsilon$\n",
    "\n",
    "ii. $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon$\n",
    "\n",
    "iii. $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$ \n",
    "\n",
    "iv. $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 X^4 + \\epsilon$ \n",
    "\n",
    "Note: for linear regression, the LOOCV error can be computed via the following short-cut formula:\n",
    "\n",
    "$$  \\text{LOOCV Error} = \\frac{1}{n}\\sum_{i=1}^{n} \\bigg( \\frac{Y_i - \\widehat{Y_i}}{1-H_{ii}} \\bigg)^2$$\n",
    "\n",
    "where $H_{ii}$ is the $i^\\text{th}$ diagonal entry of the projection matrix $H = \\textbf{X}(\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T$, and $\\textbf{X}$ is a matrix of predictors (the design matrix). This formula is an alternative to actually carrying out the $n = 100$ regressions you would otherwise need for LOOCV. An example of how to calculate the projection matrix $H$ is provided below for the case of $n=5$ and the model $Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon$ . To get the diagonal elements of $H$ you may find the function `np.diag()` useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_x = np.array([-3, -4, -5, -6, -7])        #generate the x variable\n",
    "design_x = np.vander(example_x, 3)          #calculate the design matrix for the polynomial model with 3 fit parameters\n",
    "H = np.dot(design_x, np.dot(np.linalg.inv(np.dot(design_x.T, design_x)), design_x.T))  #calculate H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## please put your answer here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d\n",
    "\n",
    "Repeat Part c using another random seed to generate data, and report your results. Are your results the same as what you got in Part c? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## please put your answer here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part e\n",
    "\n",
    "Which of the models in Part c had the smallest LOOCV error? Is this what you expected? Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## please put your answer here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part f\n",
    "\n",
    "Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in Part c using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## please put your answer here ##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
