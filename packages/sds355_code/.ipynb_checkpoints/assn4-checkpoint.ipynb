{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: PCA\n",
    "\n",
    "Due: Thursday, October 24 at midnight\n",
    "\n",
    "In this assignment you'll gain some hands-on experience with principal components analysis (PCA).\n",
    "\n",
    "The assignment has three problems. The first problem investigates PCA and linear regression on a simple toy data set. The second two problems use the MNIST and Fashion MNIST data, and the database of faces that we began looking at during last week's lectures. In the second problem, you will study how different numbers of principal components represent the images visually. For third problem you will use logistic regression to predict the class label of images using the principal components representation of the images, and examine how the classification error changes with the number of principal components used. \n",
    "\n",
    "For the second two problems, once you get your code to work on MNIST, it should be straightforward to just copy paste the code and then run it on Fashion MNIST and the face data. \n",
    "\n",
    "Please submit your notebook and pdf (from html) following the usual instructions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Principal components and Least squares\n",
    "\n",
    "In least-squares regression one of the assumptions made is that the explanatory variable(s) are non-random and contain no measurement error. Therefore, the size of the residuals (vertical distances between each observed values of the response variable and the line) completely characterize the loss due to a given line. However, it is often the case that explanatory variables do have some randomness in them, in which case we may wish to characterize the loss with the orthogonal distances between data points and the line. This can be done with what is called Principal Component Regression, which you will have some time to use in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)\n",
    "\n",
    "The cell below simulates two indepndent random variables, each from a Normal distribution with mean $0$. It then rotates the data by an angle $\\dfrac{\\pi}{3}$. What is the slope and intercept of a horizontal line after it has been rotated about the origin by $\\dfrac{\\pi}{3}$ radians? Add a line with this slope and intercept to the plot generated in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "X = np.vstack((np.random.normal(0, 1, size=100), np.random.normal(0, 0.3, size=100))).T\n",
    "\n",
    "theta = np.pi/3\n",
    "R = np.array([np.cos(theta), np.sin(theta), -np.sin(theta), np.cos(theta)]).reshape(2,2)\n",
    "X = np.dot(X, R)\n",
    "\n",
    "plt.scatter(np.array(X[:,0]), np.array(X[:,1]))\n",
    "plt.xlabel(\"X\", fontsize=12)\n",
    "plt.ylabel(\"Y\", fontsize=12)\n",
    "\n",
    "\n",
    "#Your Code Here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "\n",
    "Use least-squares regression to fit a line (with a slope and intercept) to the data generated above. Create a plot that displays the data, the true line, and the least-squares regression line. Be sure to label the two lines with legends in your plot!\n",
    "\n",
    "You could use `statsmodels.api.OLS` to fit the \"ordinary least-squares\" regression, or any other function of your choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c)\n",
    "\n",
    "Now fit a line to the data by projecting onto the first principal component. What is the slope of the line created by the first principal component, and how does it relate to the true slope? Create a plot with all three lines, including those you constructed in parts (a) and (b). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Markdown Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d)\n",
    "\n",
    "Explain why least-squares regression and principal components analysis give different fits to the data in part (c)? \n",
    "Can you say that one fit is better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems 2 and 3: MNIST and  Fashion MNIST data\n",
    "\n",
    "\n",
    "For the next two problems you will need the MNIST and Fashion MNIST data. You can download these data sets here:\n",
    "\n",
    "MNIST   http://yann.lecun.com/exdb/mnist/<br>\n",
    "FASHION-MNIST    https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion\n",
    "\n",
    "Download the following files:<br>\n",
    "train-images-idx3-ubyte.gz<br>\n",
    "train-labels-idx1-ubyte.gz<br>\n",
    "t10k-images-idx3-ubyte.gz<br>\n",
    "t10k-labels-idx1-ubyte.gz\n",
    "\n",
    "To run the code, put the data in directories named `mnist` and `fashion-mnist` within the same directory as this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following \"helper functions\" should be used to read in the MNIST and Fashion MNIST datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, gzip\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    data_dir = os.path.join(\"./\", dataset_name)\n",
    "        \n",
    "    def extract_data(filename, num_data, head_size, data_size):\n",
    "        with gzip.open(filename) as bytestream:\n",
    "            bytestream.read(head_size)\n",
    "            buf = bytestream.read(data_size * num_data)\n",
    "            data = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
    "        return data\n",
    "\n",
    "    data = extract_data(data_dir + '/train-images-idx3-ubyte.gz', 60000, 16, 28 * 28)\n",
    "    trX = data.reshape((60000, 28, 28))\n",
    "\n",
    "    data = extract_data(data_dir + '/train-labels-idx1-ubyte.gz', 60000, 8, 1)\n",
    "    trY = data.reshape((60000))\n",
    "\n",
    "    data = extract_data(data_dir + '/t10k-images-idx3-ubyte.gz', 10000, 16, 28 * 28)\n",
    "    teX = data.reshape((10000, 28, 28))\n",
    "\n",
    "    data = extract_data(data_dir + '/t10k-labels-idx1-ubyte.gz', 10000, 8, 1)\n",
    "    teY = data.reshape((10000))\n",
    "\n",
    "    trY = np.asarray(trY)\n",
    "    teY = np.asarray(teY)\n",
    "\n",
    "    X = np.concatenate((trX, teX), axis=0)\n",
    "    y = np.concatenate((trY, teY), axis=0).astype(np.int)\n",
    "\n",
    "    seed = 409\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(y)\n",
    "    return X / 255., y    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: PCA for Dimension Reduction\n",
    "\n",
    "In this problem you will approximately reconstruct images by simplifying them to multiples of a few principal components.\n",
    "\n",
    "Note: When you display the images, use the color map `cmap=plt.cm.gray.reversed()` for MNIST and Fashion MNIST and use `cmap=plt.cm.gray` for the face data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)\n",
    "\n",
    "Pick a random seed in the next cell to select a random image of a handwritten $0$ from the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data('mnist')\n",
    "x = x.reshape([70000, 28*28])\n",
    "zeros = np.where(y==0)[0]\n",
    "x = x[zeros,:]\n",
    "y = y[zeros]\n",
    "np.random.seed(13) # put your seed here\n",
    "my_image = np.random.randint(0, len(y), size=1)\n",
    "\n",
    "plt.imshow(x[my_image,:].reshape((28,28)), cmap=plt.cm.gray.reversed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k = 0, 10, 20, ...,100$, use $k$ principal components for MNIST $0$'s to approximately reconstruct the image selected above. Display the reconstruction for each value of $k$. To display the set of images compactly, you may want to use subplot, as shown in the starter code for Problem 3(b) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "\n",
    "Repeat Part (a), but this time for the dresses in the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data('fashion-mnist')\n",
    "x = x.reshape([70000, 28*28])\n",
    "zeros = np.where(y==3)[0]\n",
    "x = x[zeros,:]\n",
    "y = y[zeros]\n",
    "np.random.seed(29) #put your seed here\n",
    "my_image = np.random.randint(0, len(y), size=1)\n",
    "\n",
    "plt.imshow(x[my_image,:].reshape((28,28)), cmap=plt.cm.gray.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c)\n",
    "\n",
    "Do the same thing as in Parts (a) and (b), this time reconstructing an image of Gerhard Schroeder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_people.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lfw_people.data\n",
    "y = lfw_people.target\n",
    "zeros = np.where(y==4)[0]\n",
    "x = x[zeros,:]\n",
    "y = y[zeros]\n",
    "np.random.seed(111) #put your seed here\n",
    "my_image = np.random.randint(0, len(y), size=1)\n",
    "\n",
    "\n",
    "plt.imshow(x[my_image,:].reshape((50,37)), cmap=plt.cm.gray)#, cmap=plt.cm.gray.reversed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: PCA for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)\n",
    "\n",
    "Load in the MNIST data with the labels as `y` and the images as `x` by running the next cell. Create a subset of the data by keeping only the images that have the label of either $4$ or $9$. Use Principal Components Analysis (PCA) to project the data onto the first two principal components, and create a plot of the projected data color-coded by the label. Does the plot make sense? Explain in a couple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x, y = load_data('mnist')\n",
    "x = x.reshape([70000, 28*28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot makes sense because we see some separation between digits 4 and digits 9. However, there are some overlap in between, which indicates that 4's and 9's can be completely distinguished using the first two principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "\n",
    "Why not use more principal components! For $k = 2,3,4,...,15$, use PCA to project the data onto $k$ principal components. For each $k$, use logistic regression to build a model to classify images as $4$ or $9$, and calculate the misclassification rate. Create a plot of misclassification rate as a function of $k$, the number of principal components used. Does the plot make sense? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c)\n",
    "\n",
    "Build a logistic regression model using $10$ principal components. Create a list called `misclass` that lists the indices of all images that were misclassified with this model. Run the cell below to create a visualization of the first $16$ of these images. Does it make sense that these would be hard to classify correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here, modify the line below\n",
    "misclass = np.zeros_like(y[keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code will display the images that are misclassified\n",
    "\n",
    "plt.figure(figsize=(1.8 * 4, 2.4 * 4))\n",
    "plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(images[misclass[i]].reshape((28, 28)), cmap=plt.cm.gray.reversed())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Markdown Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that these would be hard to classify correctly because the images all look ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d)\n",
    "\n",
    "Now use the Fashion-MNIST data and train logistic regression models to classify coats ($y=4$) and handbags ($y=8$). Again use $k = 2,3,4,...,15$ to project the data onto $k$ principal components, and calculate the misclassification rate at each $k$. Create a plot of misclassification rate vs. $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data('fashion-mnist')\n",
    "fashion = x.reshape([70000, 28*28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (e)\n",
    "\n",
    "Follow the same procedure as in Parts (b) and (c) with the faces data instead of MNIST. This time, however, build a model that can classify whether an image is of George W. Bush ($y=3$) someone else. Create a variable that takes the value $1$ when the image is of George W. Bush and $0$ when it is not, and use this to train each Logistic Regression model after projecting onto the first $k$ principal components. Create a plot of misclassification rate as a function of $k$, letting $k$ vary between 2 and 30. Then, show training examples that are misclassified for the $k=30$ model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "X = lfw_people.data\n",
    "y = lfw_people.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
