{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&DS 355/555: Assignment 1\n",
    "### NetID: ????\n",
    "\n",
    "Due: Sep 17, 2019 11:59pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Simple Linear Regression (25 points)\n",
    "\n",
    "## Problem 1.a:\n",
    "\n",
    "In class we considered linear regression with the model $$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$ where $\\epsilon \\sim N(0, \\sigma^2)$ for $i = 1,2,\\ldots,n$. Suppose that we believe that the true value of $\\beta_0$ is zero. In this case we now consider the simpler model $Y_i = \\beta_1 X_i + \\epsilon_i$. Find an expression for $\\beta_1$, the estimate of $\\beta_1$ that minimizes the sum of squared residuals for this simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.b:\n",
    "\n",
    "Download the `fatherson.csv` file on Canvas with the Jupyter notebook for this homework assignment. This dataset, collected by Galton, contains the height of sons and the height of their father.\n",
    "To read it in, use the function below:\n",
    "\n",
    "```python\n",
    "x = pd.read_csv(\"fatherson.csv\")\n",
    "```\n",
    "\n",
    "After reading in this dataset, create a scatterplot of the sons’ heights (on the $Y$-axis) versus the fathers’ heights. Use your answer from (a) to calculate the slope of the least-squares line under the model with no intercept: $$\\text{Son}_i = \\beta_1 \\text{Father}_i + \\epsilon_i$$\n",
    "Add the fitted line to the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.c:\n",
    "\n",
    "Interpret the meaning of the coefficient $\\beta_1$ in the context of Galton’s father-son dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.d:\n",
    "\n",
    "Use the equations provided in class (for the least-squares coefficients of the linear regression model that includes an intercept) to calculate the least-squares estimates of the coefficients for the linear model that includes a slope and an intercept: $$\\text{Son}_i = \\beta_0 + \\beta_1 \\text{Father}_i + \\epsilon_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Linear regression and classification (30 points)\n",
    "\n",
    "Citi Bike is a public bicycle sharing system in New York City. There are hundreds of bike stations scattered throughout the city. Customers can check out a bike at any station and return it at any other station. Citi Bike caters to both commuters and tourists. Details on this program can be found at https://www.citibikenyc.com/\n",
    "\n",
    "For this problem, you will build models to predict Citi Bike usage, in number of trips per day.\n",
    "The dataset consists of Citi Bike usage information and weather data recorded from Central Park. \n",
    "\n",
    "In the `citibike_*.csv` files, we see:\n",
    "\n",
    "1. date\n",
    "\n",
    "2. trips: the total number of Citi Bike trips. This is the outcome variable.\n",
    "\n",
    "3. n_stations: the total number of Citi Bike stations in service\n",
    "\n",
    "4. holiday: whether or not the day is a work holiday\n",
    "\n",
    "5. month: taken from the date variable\n",
    "\n",
    "6. dayofweek: taken from the date variable\n",
    "\n",
    "In the `weather.csv` file, we have:\n",
    "\n",
    "1. date\n",
    "\n",
    "2. PRCP: amount precipitation (i.e. rainfall amount) in inches \n",
    "\n",
    "3. SNWD: snow depth in inches\n",
    "\n",
    "4. SNOW: snowfall in inches\n",
    "\n",
    "5. TMAX: maximum temperature for the day, in degrees F\n",
    "\n",
    "6. TMIN: minimum temperature for the day, in degrees F\n",
    "\n",
    "7. AWND: average windspeed\n",
    "\n",
    "You are provided a training set consisting of data from 7/1/2013 to 3/31/2016, and a test set consisting of data after 4/1/2016. The weather file contains weather data for the entire year. \n",
    "\n",
    "## Problem 2.a: Read in and merge the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read in the data, you can run, for example:\n",
    "```python\n",
    "train = pd.read_csv(\"citibike_train.csv\")\n",
    "test = pd.read_csv(\"citibike_test.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the training and test data with the weather data, by date. Once you have successfully merged the data, you may drop the \"date\" variable; we will not need it for the rest of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For the rest of this problem, you will train your models on the training data and evaluate them on the test data.*\n",
    "\n",
    "As always, before you start any modeling, you should look at the data. Make scatterplots of some of the numeric variables. Look for outliers and strange values. Comment on any steps you take to remove entries or otherwise process the data. Also comment on whether any predictors are strongly correlated with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.b: Linear regression\n",
    "\n",
    "Fit a linear regression model to predict the number of trips. Include all the covariates in the data. You may import the `statsmodels.api` module to get a R-like statistical output. You may write code as:\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(X) # to get the intercept term\n",
    "\n",
    "model = sm.OLS(y,X).fit()\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "Next, find the \"best\" linear model that uses only $p$ variables,\n",
    "for each $p=1,2,3,4,5$.  It is up to you to choose how to select the \"best\" subset of variables. \n",
    "(A categorical variable or factor such as \"month\" corresponds to a single variable.) Describe how you selected \n",
    "each model. Give the $R^2$ and the mean squared error (MSE) on the training and test set for each of the models. \n",
    "Which model gives the best fit to the data? Comment on your findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.c: KNN Classification\n",
    "\n",
    "Now we will transform the outcome variable to allow us to do classification. Create a new vector $Y$ with entries:\n",
    "$$\n",
    "Y[i] = \\mathbf{1} \\{ trips[i] > median(trips) \\}\n",
    "$$\n",
    "\n",
    "Use the median of the variable from the full data (training and test combined). After computing\n",
    "the binary outcome variable $Y$, you should drop the original trips variable from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in $k$-nearest neighbors classification, the predicted value $\\hat Y$ of $X$ is \n",
    "the majority vote of the labels for the $k$ nearest neighbors $X_i$ to $X$. We will use the Euclidean distance as our measure of distance between points. Note that the Euclidean distance doesn't make much sense for factor variables, so just drop the predictors that are categorical for this problem. Standardize the numeric predictors so that they have mean zero and constant standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use the ```KNeighborsClassifier``` function from the ```sklearn.neighbors``` module to perform $k$-nearest neighbor classification, using as the neighbors the labeled points in the training set. Fit a classifier for $k = 1:50$, and find the mis-classification rate on both the training and test sets for each $k$. On a single plot, show the training set error and the test set error as a function of $k$. How would you choose the optimal $k$? Comment on your findings, and in particular on the possibility of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Classification for a Gaussian Mixture (25 points)\n",
    "\n",
    "A Gaussian mixture model is a random combination of multiple Gaussians. Specifically, we can generate $n$ data points from such a distribution in the following way. First generate labels $Y_1, \\cdots, Y_n$ according to \n",
    "$$\n",
    "Y_i =\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t0  & \\mbox{with probability } 1/2 \\\\\n",
    "\t\t 1 & \\mbox{with probability } 1/2.\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "Then, generate the data $X_1, \\cdots, X_n$ according to\n",
    "$$\n",
    "X_i \\sim\n",
    "\\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\tN(\\mu_0, \\sigma_0^2)  & \\mbox{if } Y_i = 0 \\\\\n",
    "\t\tN(\\mu_1, \\sigma_1^2) & \\mbox{if } Y_i = 1.\n",
    "\t\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "Given such data $\\{X_i\\}$, we may wish to recover the true labels $Y_i$, which is a classification task.\n",
    "\n",
    "## Problem 3.a.\n",
    "\n",
    "Suppose the parameters of the above model are: $\\mu_0 = 0, \\mu_1 = 3, \\sigma_0^2 = \\sigma_1^2 = 1$. Then the Bayes classifier is given by\n",
    "$$\n",
    "f(X) = I\\{X > 1.5 \\},\n",
    "$$\n",
    "where $I$ is the indicator function (take note of the 1.5, and it's relation with the means of the two Normal distributions).\n",
    "\n",
    "Now generate $n = 2000$ data points from this dataset. Plot a histogram of the $X$'s. This histogram is meant to be a sanity check for you; it should help you verify that you've generated the data properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set aside a randomly-selected test set of $n/5$ points. We will refer to the rest of the data as the training data. Use the labels of the training data to calculate the group means. That is, calculate the mean value of all the $X_i$'s in the training data with label $Y_i = 0$. Call this sample mean $\\hat \\mu_0$. Do the same thing to find $\\hat \\mu_1$. To be explicit, let $C_j = \\{ i : Y_i = j \\}$, and define\n",
    "$$\n",
    "\\hat \\mu_j = \\frac{1}{|C_j|} \\sum_{i \\in C_j} X_i\n",
    "$$\n",
    "Now classify the data in your test set. To do this, recall that your rule in Part a. depended on the true data means $\\mu_0 = 0$ and $\\mu_1 = 3$. Plug in the sample means $\\hat \\mu_j$ instead. Evaluate the estimator's performance using the loss: \n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i = 1}^n 1\\{ \\hat Y_i \\ne Y_i \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3.b.\n",
    "\n",
    "Now you train and evaluate classifiers for training sets of increasing size $n$, as specified below. For each $n$, you should\n",
    "\n",
    "1. Generate a training set of size $n$ from the above model (with the same parameters).\n",
    "2. Generate a test set of size 10,000. Note that the test set itself will change on each round, but the size will always be the same: 10,000.\n",
    "3. Compute the sample means on the training data.\n",
    "4. Classify the test data as described in Part c.\n",
    "5. Compute the error rate.\n",
    "\n",
    "Plot the error rate as a function of $n$. Comment on your findings. What is happening to the error rate as $n$ grows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- please write your answer here. -- ## \n",
    "seq_n = np.arange(start = 2000, stop = 20000, step = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
